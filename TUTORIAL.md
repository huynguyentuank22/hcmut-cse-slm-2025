# TUTORIAL - H∆∞·ªõng d·∫´n chi ti·∫øt s·ª≠ d·ª•ng Repository

> **Repository cho cu·ªôc thi: H·ªôi thi K·ªπ thu·∫≠t AI 2025 - Th√°ch th·ª©c M√¥ h√¨nh Ng√¥n ng·ªØ Nh·ªè**  
> Khoa Khoa h·ªçc v√† K·ªπ thu·∫≠t M√°y t√≠nh - Tr∆∞·ªùng ƒêH B√°ch khoa ƒêHQG-HCM

---

## üìã M·ª•c l·ª•c

1. [Gi·ªõi thi·ªáu](#gi·ªõi-thi·ªáu)
2. [C·∫•u tr√∫c Repository](#c·∫•u-tr√∫c-repository)
3. [Datasets](#datasets)
4. [Setup m√¥i tr∆∞·ªùng](#setup-m√¥i-tr∆∞·ªùng)
5. [Workflow chi ti·∫øt](#workflow-chi-ti·∫øt)
6. [Tips & Tricks](#tips--tricks)
7. [Troubleshooting](#troubleshooting)

---

## üéØ Gi·ªõi thi·ªáu

Repository n√†y ch·ª©a to√†n b·ªô code v√† notebook ƒë·ªÉ:

- **Ph√¢n t√≠ch d·ªØ li·ªáu (EDA)**: Hi·ªÉu r√µ v·ªÅ 2 datasets VMLU v√† VNHSGE
- **Supervised Fine-Tuning (SFT)**: Train model v·ªõi Unsloth + LoRA
- **Inference & Testing**: Test model v√† t·∫°o submission

### C√¥ng ngh·ªá s·ª≠ d·ª•ng

- **Unsloth**: Framework t·ªëi ∆∞u cho vi·ªác fine-tune LLM nhanh h∆°n, √≠t RAM h∆°n
- **LoRA (Low-Rank Adaptation)**: K·ªπ thu·∫≠t PEFT ƒë·ªÉ fine-tune hi·ªáu qu·∫£
- **TRL (Transformer Reinforcement Learning)**: Library cho supervised fine-tuning
- **Hugging Face Transformers**: Th∆∞ vi·ªán x·ª≠ l√Ω model

---

## üìÅ C·∫•u tr√∫c Repository

```
hcmut-cse-slm-2025/
‚îú‚îÄ‚îÄ notebooks/                              # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ Hoi_thi_Ky_thuat_AI_2025_SFT_Unsloth_Colab.ipynb  # Main training notebook
‚îÇ   ‚îú‚îÄ‚îÄ EDA-vmlu.ipynb                     # EDA cho VMLU dataset
‚îÇ   ‚îî‚îÄ‚îÄ EDA-vnhsge.ipynb                   # EDA cho VNHSGE dataset
‚îÇ
‚îú‚îÄ‚îÄ data/                                   # Datasets v√† exported data
‚îÇ   ‚îú‚îÄ‚îÄ HTKTAI2025_example_sft_dataset.jsonl  # Dataset v√≠ d·ª• (demo only)
‚îÇ   ‚îú‚îÄ‚îÄ vmlu_mqa_v1.5/                     # VMLU dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dev.json                       # ~330 samples
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ valid.json                     # ~815 samples
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test.json                      # ~10,000 samples
‚îÇ   ‚îú‚îÄ‚îÄ VNHSGE/                            # VNHSGE dataset (not in git)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dataset/VNHSGE-V/JSON format/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ train/                     # 9 m√¥n h·ªçc (2019-2023)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ val/                       # 3 m√¥n
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test/                      # 3 m√¥n
‚îÇ   ‚îú‚îÄ‚îÄ sft_dataset_vmlu/                  # Exported SFT format
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_sft_vmlu.jsonl          # ~1,145 samples
‚îÇ   ‚îî‚îÄ‚îÄ sft_dataset_vnhsge/                # Exported SFT format
‚îÇ       ‚îú‚îÄ‚îÄ train_sft.jsonl               # Train only (text, no images, no Literature)
‚îÇ       ‚îú‚îÄ‚îÄ val_sft.jsonl                 # Validation
‚îÇ       ‚îî‚îÄ‚îÄ train_val_combined_sft.jsonl  # Combined (recommended for training)
‚îÇ
‚îú‚îÄ‚îÄ scripts/                                # Utility scripts
‚îÇ   ‚îî‚îÄ‚îÄ download-unsloth.py                # Download Unsloth cache
‚îÇ
‚îú‚îÄ‚îÄ unsloth_compiled_cache/                 # Unsloth pre-compiled trainers
‚îÇ   ‚îú‚îÄ‚îÄ UnslothSFTTrainer.py
‚îÇ   ‚îî‚îÄ‚îÄ ...                                # Other trainers (DPO, PPO, etc.)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                        # Python dependencies
‚îú‚îÄ‚îÄ .gitignore                             # Git ignore rules
‚îú‚îÄ‚îÄ README.md                              # Project overview
‚îî‚îÄ‚îÄ TUTORIAL.md                            # This file
```

---

## üìä Datasets

### 1. VMLU (Vietnamese Multi-task Language Understanding)

**ƒê·∫∑c ƒëi·ªÉm:**
- Multiple choice questions (4-5 ƒë√°p √°n: A/B/C/D/E)
- Format ƒë∆°n gi·∫£n, kh√¥ng c√≥ h√¨nh ·∫£nh
- 3 splits: dev, valid, test

**S·ªë l∆∞·ª£ng:**
- Dev: ~330 samples
- Valid: ~815 samples  
- Test: ~10,000 samples
- **Combined train**: ~1,145 samples (dev + valid)

**Format g·ªëc:**
```json
{
  "question": "C√¢u h·ªèi...",
  "choices": ["A. ...", "B. ...", "C. ...", "D. ..."],
  "answer": "A",
  "explanation": "Gi·∫£i th√≠ch..."
}
```

**Sau khi export sang SFT:**
```json
{
  "messages": [
    {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi tr·∫Øc nghi·ªám..."},
    {"role": "user", "content": "C√¢u h·ªèi: ...\nA. ...\nB. ...\nC. ...\nD. ..."},
    {"role": "assistant", "content": "{\"answer\":\"A\"}"}
  ],
  "id": "...",
  "subject": "..."
}
```

### 2. VNHSGE (Vietnamese National High School Graduation Examination)

**ƒê·∫∑c ƒëi·ªÉm:**
- ƒê·ªÅ thi THPT Qu·ªëc gia 2019-2023
- **9 m√¥n h·ªçc** trong train: Biology, Chemistry, CivicEducation, English, Geography, History, **Literature**, Mathematics, Physics
- **C√≥ h√¨nh ·∫£nh** trong m·ªôt s·ªë c√¢u h·ªèi
- **M√¥n VƒÉn (Literature)** l√† t·ª± lu·∫≠n, KH√îNG ph·∫£i multiple choice

**C·∫£nh b√°o quan tr·ªçng:**
- ‚ö†Ô∏è **B·ªé QUA m√¥n VƒÉn (Literature)** - Kh√¥ng ph·∫£i tr·∫Øc nghi·ªám
- ‚ö†Ô∏è **B·ªé QUA c√¢u h·ªèi c√≥ h√¨nh ·∫£nh** - C·∫ßn x·ª≠ l√Ω ri√™ng v·ªõi multimodal model

**S·ªë l∆∞·ª£ng (sau khi l·ªçc):**
- Train: ~h√†ng ngh√¨n samples (tr·ª´ Literature v√† c√¢u c√≥ h√¨nh)
- Val: ~h√†ng trƒÉm samples
- Test: ~h√†ng trƒÉm samples
- **Combined**: train + val (recommended)

**Format g·ªëc:**
```json
{
  "ID": "...",
  "Image_Question": "",  // Path to image or empty
  "Question": "C√¢u h·ªèi...",
  "Choice": "A",
  "Image_Answer": "",
  "Explanation": "Gi·∫£i th√≠ch..."
}
```

---

## üõ†Ô∏è Setup m√¥i tr∆∞·ªùng

### Option 1: Google Colab (Recommended)

**∆Øu ƒëi·ªÉm:**
- Mi·ªÖn ph√≠ GPU (T4)
- Kh√¥ng c·∫ßn setup local
- Ch·∫°y ngay ƒë∆∞·ª£c

**B∆∞·ªõc th·ª±c hi·ªán:**

1. M·ªü notebook trong Colab:
   - `notebooks/Hoi_thi_Ky_thuat_AI_2025_SFT_Unsloth_Colab.ipynb`
   - Ho·∫∑c link: https://colab.research.google.com/drive/1baGxyFAVQuIz7NOKu7g4miFc6liG2SQe

2. Ch·ªçn Runtime > Change runtime type > **T4 GPU**

3. Upload dataset l√™n Colab ho·∫∑c mount Google Drive

4. Ch·∫°y t·ª´ng cell theo th·ª© t·ª±

### Option 2: Local Machine

**Y√™u c·∫ßu:**
- GPU NVIDIA v·ªõi CUDA (khuy·∫øn ngh·ªã: RTX 3060+ v·ªõi 12GB+ VRAM)
- Python 3.10+
- CUDA 11.8+ ho·∫∑c 12.1+

**C√†i ƒë·∫∑t:**

```bash
# Clone repo
git clone https://github.com/huynguyentuank22/hcmut-cse-slm-2025.git
cd hcmut-cse-slm-2025

# T·∫°o virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ho·∫∑c
venv\Scripts\activate     # Windows

# C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt

# C√†i ƒë·∫∑t Unsloth
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

# Verify installation
python -c "import unsloth; print('Unsloth OK')"
```

**L∆∞u √Ω v·ªÅ GPU:**
- C·∫ßn √≠t nh·∫•t 12GB VRAM cho model 3B parameters
- N·∫øu Out of Memory, gi·∫£m `per_device_train_batch_size` ho·∫∑c `max_seq_length`

---

## üîÑ Workflow chi ti·∫øt

### B∆∞·ªõc 1: Exploratory Data Analysis (EDA)

#### 1.1. Ph√¢n t√≠ch VMLU Dataset

**File:** `notebooks/EDA-vmlu.ipynb`

**Ch·∫°y notebook ƒë·ªÉ:**
- ‚úÖ Load v√† xem c·∫•u tr√∫c d·ªØ li·ªáu
- ‚úÖ Th·ªëng k√™ ph√¢n b·ªë ƒë√°p √°n (A/B/C/D/E)
- ‚úÖ Ph√¢n t√≠ch ƒë·ªô d√†i c√¢u h·ªèi, explanation
- ‚úÖ ∆Ø·ªõc l∆∞·ª£ng tokens c·∫ßn cho training
- ‚úÖ **Export sang format SFT**: `data/sft_dataset_vmlu/train_sft_vmlu.jsonl`

**K·∫øt qu·∫£:**
- ~1,145 samples ƒë·ªÉ train
- Format chu·∫©n messages cho SFT
- ∆Ø·ªõc t√≠nh MAX_SEQ_LENGTH: ~800 tokens

#### 1.2. Ph√¢n t√≠ch VNHSGE Dataset

**File:** `notebooks/EDA-vnhsge.ipynb`

**Ch·∫°y notebook ƒë·ªÉ:**
- ‚úÖ Load t·∫•t c·∫£ JSON files (train/val/test)
- ‚úÖ Ph√¢n t√≠ch theo m√¥n h·ªçc
- ‚úÖ Ph√¢n t√≠ch c√¢u h·ªèi c√≥ h√¨nh ·∫£nh
- ‚úÖ Ph√¢n t√≠ch ƒë·ªô d√†i c√¢u h·ªèi theo m√¥n
- ‚úÖ **L·ªçc b·ªè**:
  - M√¥n VƒÉn (Literature)
  - C√¢u h·ªèi c√≥ h√¨nh ·∫£nh
- ‚úÖ **Export sang format SFT**: `data/sft_dataset_vnhsge/train_val_combined_sft.jsonl`

**K·∫øt qu·∫£:**
- H√†ng ngh√¨n samples (text-only, no Literature)
- Format chu·∫©n messages cho SFT
- ∆Ø·ªõc t√≠nh MAX_SEQ_LENGTH: ~1,200 tokens

### B∆∞·ªõc 2: Ch·ªçn Dataset v√† Chu·∫©n b·ªã

**T√πy ch·ªçn:**

| Dataset | Samples | ƒê·ªô ph·ª©c t·∫°p | MAX_SEQ_LENGTH | Khuy·∫øn ngh·ªã |
|---------|---------|-------------|----------------|-------------|
| VMLU    | ~1,145  | ƒê∆°n gi·∫£n    | 800            | Th·ª≠ nghi·ªám nhanh |
| VNHSGE  | ~nhi·ªÅu  | Ph·ª©c t·∫°p h∆°n | 1,200         | Training ch√≠nh |
| Combined| ~nhi·ªÅu  | Mix         | 1,200          | **T·ªët nh·∫•t** |

**L∆∞u √Ω:**
- File JSONL ph·∫£i ·ªü format messages (system/user/assistant)
- M·ªói line l√† 1 JSON object
- Encoding: UTF-8

### B∆∞·ªõc 3: Supervised Fine-Tuning (SFT)

**File:** `notebooks/Hoi_thi_Ky_thuat_AI_2025_SFT_Unsloth_Colab.ipynb`

#### 3.1. C·∫•u h√¨nh quan tr·ªçng

```python
# Model selection
MODEL_NAME = "unsloth/Llama-3.2-3B-Instruct"  # Ho·∫∑c model kh√°c

# Dataset path
JSONL_PATH = "data/sft_dataset_vnhsge/train_val_combined_sft.jsonl"

# Training hyperparameters
MAX_SEQ_LENGTH = 1200  # D·ª±a tr√™n EDA
PER_DEVICE_BATCH_SIZE = 2  # T√πy VRAM
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 2e-4
NUM_TRAIN_EPOCHS = 3

# LoRA config
LORA_R = 16  # Rank
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
```

#### 3.2. C√°c b∆∞·ªõc training

**Cell 1-3: Setup**
```bash
# Install dependencies
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps trl peft accelerate bitsandbytes
```

**Cell 4: Load Model + LoRA**
```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,  # Auto-detect
    load_in_4bit=True,  # Ti·∫øt ki·ªám VRAM
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
)
```

**Cell 5: Load Dataset**
```python
from datasets import load_dataset

dataset = load_dataset("json", data_files={"train": JSONL_PATH})
print(f"Loaded {len(dataset['train'])} samples")
```

**Cell 6: Training v·ªõi SFTTrainer**
```python
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    dataset_text_field="messages",
    max_seq_length=MAX_SEQ_LENGTH,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=50,
        num_train_epochs=3,
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=10,
        output_dir="outputs",
        optim="adamw_8bit",
        seed=42,
    ),
)

# B·∫Øt ƒë·∫ßu training!
trainer_stats = trainer.train()
```

**Cell 7-8: L∆∞u Model**
```python
# Save LoRA adapters only
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")

# Ho·∫∑c merge LoRA v√†o base model
model.save_pretrained_merged("model_merged", tokenizer, save_method="merged_16bit")
```

**Cell 9: Upload l√™n Hugging Face Hub**
```python
from huggingface_hub import login
from getpass import getpass

# ‚ö†Ô∏è QUAN TR·ªåNG: D√πng getpass, KH√îNG hardcode token!
hf_token = getpass("Enter your HF token: ")
login(token=hf_token)

model.push_to_hub_merged(
    "your-username/model-name",
    tokenizer,
    save_method="merged_16bit",
    token=hf_token,
)
```

### B∆∞·ªõc 4: Inference & Testing

**Cell 10-11: Test model**
```python
FastLanguageModel.for_inference(model)

# Test prompt
system = 'B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi tr·∫Øc nghi·ªám. Ch·ªâ tr·∫£ JSON: {"answer":"A"} ho·∫∑c B/C/D.'
user = """C√¢u h·ªèi: 2 + 2 = ?
A. 3
B. 4
C. 5
D. 6"""

messages = [
    {"role": "system", "content": system},
    {"role": "user", "content": user}
]

# Generate
inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")
outputs = model.generate(input_ids=inputs, max_new_tokens=64, temperature=0.0)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)

# Expected output: {"answer":"B"}
```

---

## üí° Tips & Tricks

### Training Tips

1. **B·∫Øt ƒë·∫ßu v·ªõi dataset nh·ªè**
   - Train v·ªõi VMLU tr∆∞·ªõc (~1K samples)
   - Ki·ªÉm tra loss v√† output quality
   - Sau ƒë√≥ scale l√™n VNHSGE

2. **Hyperparameter tuning**
   ```python
   # Learning rate
   2e-4  # Good starting point
   1e-4  # N·∫øu loss dao ƒë·ªông nhi·ªÅu
   5e-4  # N·∫øu train ch·∫≠m
   
   # Batch size (t√πy VRAM)
   batch_size = 2  # 12GB VRAM
   batch_size = 4  # 24GB VRAM
   batch_size = 8  # 40GB VRAM
   
   # Gradient accumulation
   # Effective batch size = batch_size * accumulation_steps
   accumulation_steps = 4  # ‚Üí effective batch = 8
   ```

3. **MAX_SEQ_LENGTH optimization**
   - D·ª±a v√†o EDA ƒë·ªÉ ch·ªçn
   - P95 c·ªßa ƒë·ªô d√†i c√¢u h·ªèi + explanation
   - Th√™m buffer 20-30%
   - C√†ng nh·ªè ‚Üí c√†ng nhanh, √≠t VRAM

4. **LoRA rank tuning**
   ```python
   r = 8   # Fast, √≠t parameters
   r = 16  # Balanced (recommended)
   r = 32  # More capacity, slower
   ```

5. **Monitor training**
   - Xem loss gi·∫£m d·∫ßn
   - N·∫øu loss kh√¥ng gi·∫£m ‚Üí tƒÉng learning rate
   - N·∫øu loss dao ƒë·ªông ‚Üí gi·∫£m learning rate
   - Test inference sau m·ªói checkpoint

### Output Format Tips

**Format JSON-only:**
```json
{"answer":"A"}
```

**Format v·ªõi reasoning (t·ªët h∆°n):**
```
<think>
Ph√¢n t√≠ch c√¢u h·ªèi...
ƒê√°p √°n A v√¨...
</think>
{"answer":"A"}
```

**System prompt quan tr·ªçng:**
```python
system = '''B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi tr·∫Øc nghi·ªám. 
N·∫øu c·∫ßn suy nghƒ©, ƒë·∫∑t trong <think>...</think>. 
D√íNG CU·ªêI PH·∫¢I l√† JSON duy nh·∫•t: {"answer":"A"} ho·∫∑c B/C/D.'''
```

### Data Tips

1. **Augmentation ideas**
   - Shuffle th·ª© t·ª± A/B/C/D
   - Paraphrase c√¢u h·ªèi
   - Th√™m noise nh·∫π

2. **Balance dataset**
   - Check ph√¢n b·ªë A/B/C/D
   - N·∫øu imbalance ‚Üí oversample ho·∫∑c undersample

3. **Combine datasets**
   ```python
   # Mix VMLU + VNHSGE
   vmlu = load_dataset("json", data_files="vmlu.jsonl")
   vnhsge = load_dataset("json", data_files="vnhsge.jsonl")
   combined = concatenate_datasets([vmlu, vnhsge])
   ```

### Git Tips

1. **Tr√°nh commit token**
   - D√πng `.env` file cho secrets
   - Ho·∫∑c d√πng `getpass()` trong notebook
   - KH√îNG hardcode token!

2. **Tr√°nh commit file l·ªõn**
   - Dataset > 50MB ‚Üí Git LFS ho·∫∑c download link
   - Model checkpoints ‚Üí Hugging Face Hub
   - ƒê√£ c√≥ `.gitignore` ƒë·ªÉ tr√°nh

3. **Commit messages**
   ```bash
   git commit -m "Add: EDA notebook for VNHSGE"
   git commit -m "Fix: Remove Literature from training"
   git commit -m "Update: Improve system prompt"
   ```

---

## üîß Troubleshooting

### Common Errors

#### 1. Out of Memory (OOM)

**Tri·ªáu ch·ª©ng:**
```
RuntimeError: CUDA out of memory
```

**Gi·∫£i ph√°p:**
```python
# Gi·∫£m batch size
per_device_train_batch_size = 1

# Gi·∫£m MAX_SEQ_LENGTH
MAX_SEQ_LENGTH = 800  # thay v√¨ 1200

# TƒÉng gradient accumulation
gradient_accumulation_steps = 8

# D√πng 4-bit quantization
load_in_4bit = True

# Enable gradient checkpointing
use_gradient_checkpointing = "unsloth"
```

#### 2. Token Not Found Error

**Tri·ªáu ch·ª©ng:**
```
KeyError: 'HF_TOKEN' or Authentication failed
```

**Gi·∫£i ph√°p:**
```python
# D√πng getpass thay v√¨ hardcode
from getpass import getpass
hf_token = getpass("Enter HF token: ")

# Ho·∫∑c d√πng CLI
!huggingface-cli login
```

#### 3. Dataset Format Error

**Tri·ªáu ch·ª©ng:**
```
KeyError: 'messages' or Invalid format
```

**Gi·∫£i ph√°p:**
- Ki·ªÉm tra format JSONL ƒë√∫ng ch∆∞a
- M·ªói line ph·∫£i l√† valid JSON
- Ph·∫£i c√≥ field `messages` v·ªõi list of dicts
- M·ªói dict ph·∫£i c√≥ `role` v√† `content`

```python
# Validate format
import json

with open("dataset.jsonl") as f:
    for i, line in enumerate(f):
        try:
            data = json.loads(line)
            assert "messages" in data
            assert isinstance(data["messages"], list)
            for msg in data["messages"]:
                assert "role" in msg and "content" in msg
        except Exception as e:
            print(f"Line {i}: {e}")
```

#### 4. Loss Not Decreasing

**Tri·ªáu ch·ª©ng:**
- Loss cao v√† kh√¥ng gi·∫£m
- Model output gibberish

**Gi·∫£i ph√°p:**
1. **Ki·ªÉm tra data**
   - Xem m·∫´u dataset c√≥ ƒë√∫ng kh√¥ng
   - Ki·ªÉm tra tokenization

2. **TƒÉng learning rate**
   ```python
   learning_rate = 5e-4  # thay v√¨ 2e-4
   ```

3. **Train l√¢u h∆°n**
   ```python
   num_train_epochs = 5  # thay v√¨ 3
   ```

4. **Ki·ªÉm tra system prompt**
   - C√≥ r√µ r√†ng kh√¥ng?
   - Model c√≥ hi·ªÉu instruction kh√¥ng?

#### 5. Model Output Wrong Format

**Tri·ªáu ch·ª©ng:**
- Output kh√¥ng ph·∫£i JSON
- Thi·∫øu d·∫•u ngo·∫∑c
- C√≥ text th·ª´a

**Gi·∫£i ph√°p:**
1. **C·∫£i thi·ªán system prompt**
   ```python
   system = '''B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi tr·∫Øc nghi·ªám.
   QUY T·∫ÆC NGHI√äM NG·∫∂T:
   - D√íNG CU·ªêI C√ôNG ph·∫£i l√† JSON: {"answer":"A"}
   - Ch·ªâ tr·∫£ A, B, C, ho·∫∑c D
   - KH√îNG vi·∫øt th√™m g√¨ sau JSON
   '''
   ```

2. **Parse output c·∫©n th·∫≠n**
   ```python
   import re
   import json
   
   def extract_answer(text):
       # T√¨m JSON trong output
       match = re.search(r'\{"answer":"([A-D])"\}', text)
       if match:
           return match.group(1)
       
       # Fallback: t√¨m pattern kh√°c
       match = re.search(r'[Aa]nswer[:\s]+([A-D])', text)
       if match:
           return match.group(1)
       
       return None
   ```

3. **Constrained generation**
   ```python
   # D√πng logits processor ƒë·ªÉ force JSON format
   # (Advanced, c·∫ßn custom code)
   ```

---

## üìö Resources & References

### Documentation
- [Unsloth GitHub](https://github.com/unslothai/unsloth)
- [TRL Documentation](https://huggingface.co/docs/trl)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)

### Tutorials
- [Unsloth Colab Notebooks](https://github.com/unslothai/unsloth#-notebooks)
- [Fine-tuning LLMs Guide](https://huggingface.co/blog/fine-tune-llms)

### Competition Info
- Khoa Khoa h·ªçc v√† K·ªπ thu·∫≠t M√°y t√≠nh - HCMUT
- H·ªôi thi K·ªπ thu·∫≠t AI 2025

---

## ü§ù Contributing

N·∫øu b·∫°n t√¨m th·∫•y bug ho·∫∑c c√≥ ƒë·ªÅ xu·∫•t c·∫£i thi·ªán:

1. Fork repo
2. T·∫°o branch m·ªõi: `git checkout -b feature/improvement`
3. Commit changes: `git commit -m "Add: new feature"`
4. Push to branch: `git push origin feature/improvement`
5. T·∫°o Pull Request

---

## ‚ö†Ô∏è Disclaimer

- Dataset v√≠ d·ª• ch·ªâ ƒë·ªÉ demo pipeline
- Model v√† code ch·ªâ ph·ª•c v·ª• h·ªçc t·∫≠p
- Kh√¥ng ƒë·∫£m b·∫£o k·∫øt qu·∫£ cu·ªôc thi
- S·ª≠ d·ª•ng c√≥ tr√°ch nhi·ªám!

---

## üìß Contact & Support

N·∫øu c√≥ v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t:
1. Ki·ªÉm tra [Troubleshooting](#troubleshooting) section
2. Xem l·∫°i c√°c notebook c√≥ comment chi ti·∫øt
3. Check GitHub Issues c·ªßa Unsloth
4. Li√™n h·ªá BTC cu·ªôc thi

---

**Good luck v·ªõi cu·ªôc thi! üöÄ**
