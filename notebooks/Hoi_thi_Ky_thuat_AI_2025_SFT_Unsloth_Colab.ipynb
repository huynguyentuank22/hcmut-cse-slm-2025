{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f1f75a1",
      "metadata": {
        "id": "0f1f75a1"
      },
      "source": [
        "# H·ªôi thi K·ªπ thu·∫≠t AI 2025: Th√°ch th·ª©c M√¥ h√¨nh Ng√¥n ng·ªØ Nh·ªè ‚Äî SFT b·∫±ng Unsloth\n",
        "\n",
        "Notebook n√†y l√† **h∆∞·ªõng d·∫´n finetune SFT** ph·ª•c v·ª• cu·ªôc thi **‚ÄúH·ªôi thi K·ªπ thu·∫≠t AI 2025: Th√°ch th·ª©c M√¥ h√¨nh Ng√¥n ng·ªØ Nh·ªè‚Äù**.\n",
        "\n",
        "B·∫°n s·∫Ω l√†m ƒë∆∞·ª£c:\n",
        "1) Fine-tune **SFT (Supervised Fine-Tuning)** tr√™n dataset JSONL ki·ªÉu `messages`  \n",
        "2) Xu·∫•t ra **LoRA Adapter** (nh·∫π, d·ªÖ iterate)  \n",
        "3) **MERGE LoRA ‚Üí model ƒë√£ g·ªôp (.safetensors)** ƒë·ªÉ ƒë·∫©y l√™n **Hugging Face repo** (h·ªá th·ªëng n·ªôp b√†i ch·ªâ c·∫ßn repo n√†y)\n",
        "\n",
        "---\n",
        "\n",
        "## Nhi·ªÅu c√°ch!\n",
        "\n",
        "Trong cu·ªôc thi, model ph·∫£i tr·∫£ l·ªùi **ƒë√∫ng format** (JSON duy nh·∫•t). C√≥ nhi·ªÅu h∆∞·ªõng SFT:\n",
        "\n",
        "### C√°ch A ‚Äî SFT ‚ÄúJSON-only‚Äù\n",
        "- Data √©p assistant tr·∫£ v·ªÅ ƒë√∫ng `{\"answer\":\"A\"}` (ho·∫∑c B/C/D)\n",
        "\n",
        "### C√°ch B ‚Äî SFT ‚ÄúReasoning model‚Äù v·ªõi `<think> ... </think>`\n",
        "- Assistant sinh ra:\n",
        "  - ph·∫ßn suy nghƒ© trong `<think>...</think>`\n",
        "  - ph·∫ßn tr·∫£ l·ªùi cu·ªëi: JSON (ƒë·ªÉ ch·∫•m)\n",
        "- Tr√™n h·ªá th·ªëng n·ªôp b√†i: **h·ªá th·ªëng s·∫Ω t·ª± l∆∞·ª£c b·ªè `<think>...</think>`**, n√™n ch·∫•m v·∫´n d·ª±a tr√™n JSON cu·ªëi.\n",
        "\n",
        "> Tip: Khi t·∫°o dataset, b·∫°n nh·ªõ ‚Äúkh√≥a‚Äù format output: **JSON ph·∫£i l√† d√≤ng cu·ªëi** v√† kh√¥ng th√™m ch·ªØ th·ª´a.\n",
        "\n",
        "### C√°ch C ‚Äî SFT + System Prompt ‚Äúc·ª©ng‚Äù\n",
        "- Kh√¥ng ch·ªâ d·ª±a v√†o data, m√† c√≤n thi·∫øt k·∫ø system prompt c·ª±c ch·∫∑t:\n",
        "  - ‚ÄúCh·ªâ tr·∫£ JSON‚Äù\n",
        "  - ‚ÄúKh√¥ng ƒë∆∞·ª£c gi·∫£i th√≠ch‚Äù\n",
        "  - ‚ÄúN·∫øu thi·∫øu th√¥ng tin th√¨ v·∫´n ph·∫£i ch·ªçn A/B/C/D theo best guess‚Äù\n",
        "- R·∫•t h·ªØu √≠ch khi b·∫°n c√≥ quy·ªÅn ch·ªânh system prompt ·ªü l√∫c thi.\n",
        "\n",
        "---\n",
        "\n",
        "## 0) Chu·∫©n b·ªã runtime\n",
        "\n",
        "- Colab: `Runtime -> Change runtime type -> GPU`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bd23845e",
      "metadata": {
        "id": "bd23845e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367fea4f-e282-4baa-ab37-47ef317463d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 16 02:28:15 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0             29W /   70W |    6972MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95cbdd04",
      "metadata": {
        "id": "95cbdd04"
      },
      "source": [
        "## 1) C√†i th∆∞ vi·ªán\n",
        "\n",
        "- `unsloth`: load model + LoRA + merge nhanh\n",
        "- `trl`: SFTTrainer\n",
        "- `datasets`: load JSONL\n",
        "- `huggingface_hub`: login & push model l√™n Hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f97d5a31",
      "metadata": {
        "id": "f97d5a31"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U \"unsloth\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e599ad3",
      "metadata": {
        "id": "9e599ad3"
      },
      "source": [
        "## 2) (Tu·ª≥ ch·ªçn) Mount Google Drive\n",
        "\n",
        "N·∫øu dataset `.jsonl` ƒë·ªÉ trong Drive th√¨ mount ƒë·ªÉ ƒë·ªçc cho ti·ªán.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ac5e8896",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ac5e8896",
        "outputId": "539d3ab3-330c-4d8a-b009-f566d57913c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57e1abee",
      "metadata": {
        "id": "57e1abee"
      },
      "source": [
        "## 3) C·∫•u h√¨nh (model + data + hyperparams)\n",
        "\n",
        "### Model test trong notebook n√†y: `Qwen/Qwen3-1.7B`\n",
        "\n",
        "B·∫°n c√≥ th·ªÉ ƒë·ªïi sang model kh√°c cho ƒë√∫ng lu·∫≠t / ƒë√∫ng y√™u c·∫ßu BTC.\n",
        "N·∫øu d√πng model ‚Äúkh√¥ng n·∫±m trong danh s√°ch m·∫∑c ƒë·ªãnh‚Äù, th∆∞·ªùng c·∫ßn xin duy·ªát tr∆∞·ªõc ‚Äî nh∆∞ng v·∫´n ph·∫£i **< 2B params**.\n",
        "\n",
        "### Dataset JSONL\n",
        "M·ªói d√≤ng 1 sample:\n",
        "\n",
        "```json\n",
        "{\"messages\":[\n",
        "  {\"role\":\"system\",\"content\":\"...\"},\n",
        "  {\"role\":\"user\",\"content\":\"...\"},\n",
        "  {\"role\":\"assistant\",\"content\":\"<think>...</think>\\n{\"answer\":\"B\"}\"}\n",
        "]}\n",
        "```\n",
        "\n",
        "- N·∫øu b·∫°n kh√¥ng train reasoning: b·ªè `<think>` ƒëi, ch·ªâ ƒë·ªÉ JSON.\n",
        "- N·∫øu b·∫°n train reasoning: **ƒë·∫£m b·∫£o JSON v·∫´n l√† output cu·ªëi**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "42e2a240",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42e2a240",
        "outputId": "d37ee247-7a33-4a81-f97b-78b62529134f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSONL_PATH: /content/drive/MyDrive/hcmut-cse-slm-2025/HTKTAI2025_example_sft_dataset.jsonl\n",
            "BASE_MODEL: Qwen/Qwen3-1.7B\n",
            "DTYPE: float16 | LOAD_IN_4BIT: False\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# ====== PATHS ======\n",
        "# TODO: s·ª≠a path JSONL cho ƒë√∫ng ch·ªó b·∫°n l∆∞u, l∆∞u √Ω dataset m·∫´u kh√¥ng ph·ª•c v·ª• cho ƒë·ªÅ thi nh√©, b·∫°n c·∫ßn ƒë·ªïi n·ªôi dung l·∫°i v√¨ n√≥ ch·ªâ gi√∫p b·∫°n ch·∫°y c·∫£ pipelien th·ª≠ th√¥i\n",
        "JSONL_PATH = \"/content/drive/MyDrive/hcmut-cse-slm-2025/HTKTAI2025_example_sft_dataset.jsonl\" #\"/content/drive/MyDrive/full_data_sft.jsonl\"  # ho·∫∑c\n",
        "\n",
        "# ====== BASE MODEL (the model you asked to test in this notebook) ======\n",
        "BASE_MODEL = \"Qwen/Qwen3-1.7B\"\n",
        "\n",
        "# ====== TRAINING ======\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "NUM_EPOCHS = 3\n",
        "LR = 2e-4\n",
        "WARMUP_STEPS = 100\n",
        "\n",
        "# Batch (t√πy GPU)\n",
        "PER_DEVICE_BATCH = 2\n",
        "GRAD_ACCUM = 8\n",
        "\n",
        "# Outputs\n",
        "OUTPUT_DIR_LORA = \"/content/drive/MyDrive/hcmut-cse-slm-2025/outputs_lora_adapter\"        # LoRA adapter (nh·∫π)\n",
        "OUTPUT_DIR_MERGED = \"/content/drive/MyDrive/hcmut-cse-slm-2025/hf_repo_merged_model\"      # model ƒë√£ merge ƒë·ªÉ push l√™n HF\n",
        "\n",
        "# Memory\n",
        "LOAD_IN_4BIT = False  # Ch·ªâ n√™n ƒë·ªÉ n·∫øu nh∆∞ h·∫øt GPU memory nhe\n",
        "\n",
        "# dtype\n",
        "DTYPE = \"float16\" # ƒë·ªïi sang  bfloat16 cho c√°c gpu m·ªõi h∆°n nh√©!\n",
        "\n",
        "print(\"JSONL_PATH:\", JSONL_PATH)\n",
        "print(\"BASE_MODEL:\", BASE_MODEL)\n",
        "print(\"DTYPE:\", DTYPE, \"| LOAD_IN_4BIT:\", LOAD_IN_4BIT)\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ee766d",
      "metadata": {
        "id": "80ee766d"
      },
      "source": [
        "## 4) Load model + b·∫≠t LoRA (Unsloth)\n",
        "\n",
        "Sau khi train xong, output m·∫∑c ƒë·ªãnh b·∫°n nh·∫≠n ƒë∆∞·ª£c l√† **LoRA adapter**.  \n",
        "Mu·ªën n·ªôp b√†i b·∫°n **merge** ra model ƒë√£ g·ªôp ·ªü b∆∞·ªõc sau.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5314cc73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5314cc73",
        "outputId": "bb847150-17cb-45f0-c530-91c8de763de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.5: Fast Qwen3 patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Loaded model + LoRA OK\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = BASE_MODEL,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    dtype          = DTYPE,\n",
        "    load_in_4bit   = LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,  # b·∫≠t ƒë·ªÉ ƒë·ª° VRAM\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "# Chat template: Qwen family th∆∞·ªùng d√πng ·ªïn v·ªõi \"qwen-2.5\"\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
        "\n",
        "print(\"Loaded model + LoRA OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9d16bc",
      "metadata": {
        "id": "db9d16bc"
      },
      "source": [
        "## 5) Load dataset JSONL & format b·∫±ng chat_template\n",
        "\n",
        "Ch√∫ng ta map `messages` ‚Üí `text` b·∫±ng `tokenizer.apply_chat_template(...)` ƒë·ªÉ SFTTrainer train ƒë√∫ng format h·ªôi tho·∫°i.\n",
        "\n",
        "Preview v√†i sample ƒë·ªÉ ch·∫Øc ch·∫Øn:\n",
        "- C√≥ system/user/assistant ƒë·∫ßy ƒë·ªß\n",
        "- N·∫øu c√≥ `<think>...</think>` th√¨ JSON v·∫´n n·∫±m cu·ªëi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fe20e2a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe20e2a5",
        "outputId": "0dbbeacc-f8ca-467f-940c-8e09e3947157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 2\n",
            "\n",
            "===== PREVIEW 2 SAMPLES =====\n",
            "\n",
            "--- Sample 1 ---\n",
            "<|im_start|>system\n",
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI gi√°o d·ª•c, tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p v√† lu√¥n k√®m ph·∫ßn suy lu·∫≠n n·ªôi b·ªô trong <think>...</think> tr∆∞·ªõc khi ƒë∆∞a ra k·∫øt lu·∫≠n. B·∫°n c√≥ th·ªÉ linh ho·∫°t x·ª≠ l√Ω nhi·ªÅu d·∫°ng y√™u c·∫ßu kh√°c nhau, ch·∫≥ng h·∫°n nh∆∞ h·ªó tr·ª£ t√¢m l√Ω c·∫£m x√∫c, t∆∞ v·∫•n an to√†n, gi·∫£i th√≠ch ki·∫øn th·ª©c, t∆∞∆°ng t√°c h·ªçc t·∫≠p, t·∫°o t√†i li·ªáu gi·∫£ng d·∫°y ho·∫∑c ch·∫•m ƒëi·ªÉm t·ª± ƒë·ªông, v·ªõi vƒÉn phong r√µ r√†ng v√† ph√π h·ª£p m√¥i tr∆∞·ªùng gi√°o d·ª•c.<|im_end|>\n",
            "<|im_start|>user\n",
            "Th·∫ßy/c√¥ ∆°i, t·∫°o gi√∫p em m·ªôt gi√°o √°n 1 ti·∫øt (45 ph√∫t) cho b√†i 'S√≥ng ƒëi·ªán t·ª´', V·∫≠t l√≠ l·ªõp 12 ƒë∆∞·ª£c kh√¥ng ·∫°?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<think>\n",
            "B∆∞·ªõc 1: Ph√¢n t√≠ch y√™u c·∫ßu. Ng∆∞·ªùi d√πng l√† gi√°o vi√™n, c·∫ßn m·ªôt gi√°o √°n 45 ph√∫t cho b√†i \"S√≥ng ƒëi·ªán t·ª´\" l·ªõp 12. ƒê√¢y l√† m·ªôt b√†i quan tr·ªçng, mang t√≠nh l√Ω thuy·∫øt cao trong ch∆∞∆°ng S√≥ng √°nh s√°ng.\n",
            "B∆∞·ªõc 2: X√°c ƒë·ªãnh m·ª•c ti√™u b√†i h·ªçc.\n",
            "- V·ªÅ ki·∫øn th·ª©c: HS ph·∫£i tr√¨nh b√†y ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a, c√°c ƒë·∫∑c ƒëi·ªÉm c·ªßa s√≥ng ƒëi·ªán t·ª´ v√† thang s√≥ng ƒëi·ªán t·ª´. N·∫Øm ƒë∆∞·ª£c s·ª± lan truy·ªÅn c·ªßa s√≥ng ƒëi·ªán t·ª´ trong kh√¥ng gian.\n",
            "- V·ªÅ k·ªπ nƒÉng: V·∫≠n d·ª•ng ki·∫øn th·ª©c ƒë·ªÉ gi·∫£i th√≠ch m·ªôt s·ªë hi·ªán t∆∞·ª£ng th·ª±c t·∫ø (v√≠ d·ª•: s√≥ng radio, wifi). So s√°nh ƒë∆∞·ª£c c√°c lo·∫°i s√≥ng trong thang s√≥ng ƒëi·ªán t·ª´.\n",
            "- V·ªÅ nƒÉng l·ª±c: Ph√°t tri·ªÉn nƒÉng l·ª±c t·ª± h·ªçc, nƒÉng l·ª±c gi·∫£i quy·∫øt v\n",
            "...\n",
            "\n",
            "--- Sample 2 ---\n",
            "<|im_start|>system\n",
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI gi√°o d·ª•c, tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p v√† lu√¥n k√®m ph·∫ßn suy lu·∫≠n n·ªôi b·ªô trong <think>...</think> tr∆∞·ªõc khi ƒë∆∞a ra k·∫øt lu·∫≠n. B·∫°n c√≥ th·ªÉ linh ho·∫°t x·ª≠ l√Ω nhi·ªÅu d·∫°ng y√™u c·∫ßu kh√°c nhau, ch·∫≥ng h·∫°n nh∆∞ h·ªó tr·ª£ t√¢m l√Ω c·∫£m x√∫c, t∆∞ v·∫•n an to√†n, gi·∫£i th√≠ch ki·∫øn th·ª©c, t∆∞∆°ng t√°c h·ªçc t·∫≠p, t·∫°o t√†i li·ªáu gi·∫£ng d·∫°y ho·∫∑c ch·∫•m ƒëi·ªÉm t·ª± ƒë·ªông, v·ªõi vƒÉn phong r√µ r√†ng v√† ph√π h·ª£p m√¥i tr∆∞·ªùng gi√°o d·ª•c.<|im_end|>\n",
            "<|im_start|>user\n",
            "Th·∫ßy/c√¥ ∆°i, gi√∫p em t·∫°o m·ªôt gi√°o √°n b√†i gi·∫£ng 'Ph√©p c·ªông ph√¢n s·ªë' cho h·ªçc sinh l·ªõp 4, th·ªùi l∆∞·ª£ng 1 ti·∫øt (40-45 ph√∫t).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<think>\n",
            "B∆∞·ªõc 1: X√°c ƒë·ªãnh m·ª•c ti√™u b√†i d·∫°y. H·ªçc sinh l·ªõp 4 c·∫ßn n·∫Øm ƒë∆∞·ª£c quy t·∫Øc c·ªông hai ph√¢n s·ªë c√πng m·∫´u s·ªë v√† kh√°c m·∫´u s·ªë. \n",
            "- Ki·∫øn th·ª©c: N√™u ƒë∆∞·ª£c quy t·∫Øc c·ªông hai ph√¢n s·ªë c√πng m·∫´u s·ªë. Bi·∫øt c√°ch quy ƒë·ªìng m·∫´u s·ªë ƒë·ªÉ c·ªông hai ph√¢n s·ªë kh√°c m·∫´u s·ªë.\n",
            "- K·ªπ nƒÉng: Th·ª±c hi·ªán th√†nh th·∫°o ph√©p c·ªông hai ph√¢n s·ªë (c√πng v√† kh√°c m·∫´u). V·∫≠n d·ª•ng gi·∫£i b√†i to√°n c√≥ l·ªùi vƒÉn.\n",
            "- Ph·∫©m ch·∫•t: C·∫©n th·∫≠n, ch√≠nh x√°c khi t√≠nh to√°n. H·ª£p t√°c v·ªõi b·∫°n trong nh√≥m.\n",
            "\n",
            "B∆∞·ªõc 2: X√¢y d·ª±ng c·∫•u tr√∫c gi√°o √°n theo 5 ho·∫°t ƒë·ªông ch√≠nh.\n",
            "1. Kh·ªüi ƒë·ªông (5 ph√∫t): Tr√≤ ch∆°i nh·ªè √¥n l·∫°i ki·∫øn th·ª©c v·ªÅ ph√¢n s·ªë.\n",
            "2. H√¨nh th√†nh ki·∫øn th·ª©c (15 ph√∫t): H∆∞·ªõng d\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": JSONL_PATH}, split=\"train\").shuffle(seed=3407)\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            c,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,  # SFT: kh√¥ng th√™m prompt sinh ti·∫øp\n",
        "        )\n",
        "        for c in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True, num_proc=2)\n",
        "\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "print(\"\\n===== PREVIEW 2 SAMPLES =====\")\n",
        "for i in range(2):\n",
        "    print(f\"\\n--- Sample {i+1} ---\\n{dataset[i]['text'][:1200]}\\n...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72105bf4",
      "metadata": {
        "id": "72105bf4"
      },
      "source": [
        "## 6) Train SFT\n",
        "\n",
        "G·ª£i √Ω ch·ªânh nhanh n·∫øu OOM:\n",
        "- Gi·∫£m `PER_DEVICE_BATCH`\n",
        "- TƒÉng `GRAD_ACCUM`\n",
        "- Gi·∫£m `MAX_SEQ_LENGTH`\n",
        "- B·∫≠t `LOAD_IN_4BIT=True` (ƒë√£ b·∫≠t m·∫∑c ƒë·ªãnh cho Colab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cffe31ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "cffe31ed",
        "outputId": "0dab5fd7-9a57-4ded-a235-eacc2dede6ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n",
            "WARNING:datasets.arrow_dataset:num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2 | Num Epochs = 3 | Total steps = 3\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 8,716,288 of 1,729,291,264 (0.50% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training done!\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = SFTConfig(\n",
        "        gradient_accumulation_steps = GRAD_ACCUM,\n",
        "        learning_rate = LR,\n",
        "        num_train_epochs = NUM_EPOCHS,\n",
        "        warmup_steps = WARMUP_STEPS,\n",
        "\n",
        "        logging_steps = 10,\n",
        "        report_to = \"none\",  # kh√¥ng log ra service n√†o\n",
        "\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 500,\n",
        "        save_total_limit = 2,\n",
        "\n",
        "        fp16 = (DTYPE == \"float16\"),\n",
        "        bf16 = (DTYPE == \"bfloat16\"),\n",
        "        optim = \"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = OUTPUT_DIR_LORA,\n",
        "\n",
        "        completion_only_loss = True,  # ch·ªâ t√≠nh loss ph·∫ßn assistant\n",
        "        gradient_checkpointing=True\n",
        "    ),\n",
        ")\n",
        "\n",
        "train_stats = trainer.train()\n",
        "print(\"Training done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0c50bb1",
      "metadata": {
        "id": "c0c50bb1"
      },
      "source": [
        "## 7) L∆∞u LoRA Adapter\n",
        "\n",
        "ƒê√¢y l√† output nh·∫π, ƒë·ªÉ b·∫°n iterate nhanh (train l·∫°i / th·ª≠ prompt / debug).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cf9dd4b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf9dd4b8",
        "outputId": "0d6c1ec8-f45d-4be7-804d-0b67371db230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved LoRA adapter to: /content/drive/MyDrive/hcmut-cse-slm-2025/outputs_lora_adapter\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs(OUTPUT_DIR_LORA, exist_ok=True)\n",
        "\n",
        "trainer.model.save_pretrained(OUTPUT_DIR_LORA)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR_LORA)\n",
        "\n",
        "print(\"Saved LoRA adapter to:\", OUTPUT_DIR_LORA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ceddb6c",
      "metadata": {
        "id": "0ceddb6c"
      },
      "source": [
        "## 8) MERGE LoRA ‚Üí model ƒë√£ g·ªôp (.safetensors)\n",
        "\n",
        "V√¨ h·ªá th·ªëng n·ªôp b√†i s·∫Ω t·∫£i **model ƒë√£ merge** t·ª´ Hugging Face repo,\n",
        "ta merge ra m·ªôt folder ‚Äús·∫°ch‚Äù ƒë·ªÉ push l√™n Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b74a5545",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b74a5545",
        "outputId": "24979bdd-7f36-4ca0-acf2-88b3975c5bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.5: Fast Qwen3 patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `/content/drive/MyDrive/hcmut-cse-slm-2025/hf_repo_merged_model`: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:45<00:00, 45.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `/content/drive/MyDrive/hcmut-cse-slm-2025/hf_repo_merged_model`\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: tokenizer.model not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 8192.00it/s]\n",
            "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [02:01<00:00, 121.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/hcmut-cse-slm-2025/hf_repo_merged_model`\n",
            "Saved merged model to: /content/drive/MyDrive/hcmut-cse-slm-2025/hf_repo_merged_model\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch, os\n",
        "\n",
        "LORA_CHECKPOINT = OUTPUT_DIR_LORA\n",
        "save_dir = OUTPUT_DIR_MERGED\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "merge_dtype = \"bfloat16\" if torch.cuda.is_bf16_supported() else \"float16\"\n",
        "\n",
        "model2, tokenizer2 = FastLanguageModel.from_pretrained(\n",
        "    model_name = LORA_CHECKPOINT,\n",
        "    dtype = merge_dtype,\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "\n",
        "model2.save_pretrained_merged(\n",
        "    save_dir,\n",
        "    tokenizer2,\n",
        "    save_method = \"merged_16bit\",  # \"merged_16bit\", \"merged_4bit\", \"lora\"\n",
        ")\n",
        "\n",
        "print(\"Saved merged model to:\", save_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bae23367",
      "metadata": {
        "id": "bae23367"
      },
      "source": [
        "### (Tu·ª≥ ch·ªçn) Merge t·ª´ checkpoint LoRA c√≥ s·∫µn\n",
        "\n",
        "N·∫øu b·∫°n ƒë√£ c√≥ s·∫µn m·ªôt folder LoRA\n",
        "th√¨ ch·ªâ c·∫ßn ƒë·ªïi `LORA_CHECKPOINT`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6dc0fcfe",
      "metadata": {
        "id": "6dc0fcfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "3a0c2846-25b5-4043-c9e5-6b801a5b9be0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-211318270.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLORA_CHECKPOINT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"....\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLORA_CHECKPOINT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bfloat16\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bf16_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"float16\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, load_in_fp8, unsloth_tiled_mlp, *args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;34m\"Please separate the LoRA and base models to 2 repos.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             )\n\u001b[0;32m--> 328\u001b[0;31m         model_types = get_transformers_model_type(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mpeft_config\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/hf_utils.py\u001b[0m in \u001b[0;36mget_transformers_model_type\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m\"\"\" Gets model_type from config file - can be PEFT or normal HF \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;34mf\"Unsloth: No config file found - are you sure the `model_name` is correct?\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;34mf\"If you're using a model on your local device, confirm if the folder location exists.\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists."
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch, os\n",
        "\n",
        "LORA_CHECKPOINT = \"....\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = LORA_CHECKPOINT,\n",
        "    dtype = \"bfloat16\" if torch.cuda.is_bf16_supported() else \"float16\",\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "\n",
        "save_dir = \"hf_repo_merged_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model.save_pretrained_merged(\n",
        "    save_dir,\n",
        "    tokenizer,\n",
        "    save_method = \"merged_16bit\",\n",
        ")\n",
        "\n",
        "print(\"Saved merged model to:\", save_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e968f4b",
      "metadata": {
        "id": "2e968f4b"
      },
      "source": [
        "## 9) Push l√™n Hugging Face (repo n√†y l√† th·ª© b·∫°n n·ªôp cho h·ªá th·ªëng)\n",
        "\n",
        "### 9.1 L·∫•y token (Write)\n",
        "- V√†o Hugging Face ‚Üí `Settings` ‚Üí `Access Tokens`\n",
        "- `New token` ‚Üí ch·ªçn quy·ªÅn **Write**\n",
        "- Copy token r·ªìi paste v√†o √¥ nh·∫≠p (an to√†n: d√πng `getpass`)\n",
        "\n",
        "> M√¨nh khuy√™n b·∫°n **up l√™n h·∫øt** (√≠t nh·∫•t l√† repo merged): Colab crash kh√¥ng m·∫•t c√¥ng train l·∫°i, v√† teammate load ƒë∆∞·ª£c ngay.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "297b1cd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "297b1cd3",
        "outputId": "5aac19b1-48bb-49b8-a1bf-569ab9cc27c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF login OK\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import getpass\n",
        "\n",
        "# hf_token = getpass.getpass(\"Paste your Hugging Face token (write): \")\n",
        "hf_token = \"YOUR_HF_TOKEN_HERE\"\n",
        "login(token=hf_token)\n",
        "print(\"HF login OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb276d7",
      "metadata": {
        "id": "cdb276d7"
      },
      "source": [
        "### 9.2 T·∫°o repo & upload folder merged\n",
        "\n",
        "- Khi n·ªôp b√†i: ch·ªâ c·∫ßn ƒë∆∞a cho h·ªá th·ªëng **t√™n repo** n√†y.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "36fa452d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36fa452d",
        "outputId": "2bc02185-91e7-4059-dfc7-2dd6d895eab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Repo merged ready: huynguyentuan/htktai2025-merged-model\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, upload_folder\n",
        "import datetime\n",
        "\n",
        "api = HfApi()\n",
        "USERNAME = api.whoami()[\"name\"]\n",
        "\n",
        "REPO_MERGED = f\"{USERNAME}/htktai2025-merged-model\"\n",
        "api.create_repo(REPO_MERGED, private=False, exist_ok=True)\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=REPO_MERGED,\n",
        "    folder_path=OUTPUT_DIR_MERGED,\n",
        "    commit_message=f\"Upload merged model ({datetime.datetime.now().isoformat(timespec='seconds')})\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Repo merged ready:\", REPO_MERGED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b497dab8",
      "metadata": {
        "id": "b497dab8"
      },
      "source": [
        "## 10) Sanity check: inference ra ƒë√∫ng JSON (v√† tu·ª≥ ch·ªçn `<think>`)\n",
        "\n",
        "Test nhanh 1 c√¢u tr·∫Øc nghi·ªám:\n",
        "- N·∫øu b·∫°n train ki·ªÉu ‚ÄúJSON-only‚Äù: output ph·∫£i ch·ªâ l√† JSON.\n",
        "- N·∫øu b·∫°n train ki·ªÉu reasoning: output c√≥ th·ªÉ c√≥ `<think>...</think>` r·ªìi m·ªõi t·ªõi JSON cu·ªëi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5c50076a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c50076a",
        "outputId": "329a72bb-398d-4451-f074-8043c56abdb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.5: Fast Qwen3 patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "<think>\n",
            "Okay, let's see. The question is 2 + 2 = ? with options A. 3, B. 4, C. 5, D. 6.\n",
            "\n",
            "Hmm, adding 2 and 2. Well, 2 plus 2 is 4. So the answer should be B. Let me check the options again. Yes, B is 4. The other options are 3, 5, 6, which are all wrong. So the correct answer is B.\n",
            "</think>\n",
            "\n",
            "{\"answer\":\"B\"}<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import TextStreamer\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "test_model, test_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = OUTPUT_DIR_MERGED,\n",
        "    dtype = merge_dtype,\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "FastLanguageModel.for_inference(test_model)\n",
        "\n",
        "system = 'B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi tr·∫Øc nghi·ªám. N·∫øu suy nghƒ©, ƒë·∫∑t trong <think>...</think>. D√íNG CU·ªêI PH·∫¢I l√† JSON duy nh·∫•t: {\"answer\":\"A\"} ho·∫∑c B/C/D.'\n",
        "user = \"\"\"C√¢u h·ªèi: 2 + 2 = ?\n",
        "A. 3\n",
        "B. 4\n",
        "C. 5\n",
        "D. 6\n",
        "Ch·ªâ tr·∫£ ƒë√∫ng format.\"\"\"\n",
        "\n",
        "msgs = [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
        "prompt = test_tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "inputs = test_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "streamer = TextStreamer(test_tokenizer, skip_prompt=True)\n",
        "\n",
        "_ = test_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.0,\n",
        "    do_sample=False,\n",
        "    streamer=streamer,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41ae5c2",
      "metadata": {
        "id": "c41ae5c2"
      },
      "source": [
        "## 11) Checklist nhanh tr∆∞·ªõc khi n·ªôp\n",
        "\n",
        "- Model output **d√≤ng cu·ªëi l√† JSON** `{\"answer\":\"A\"}` / B / C / D  \n",
        "- N·∫øu d√πng `<think>`, ƒë·∫£m b·∫£o JSON v·∫´n l√† cu·ªëi (v√† platform n·ªôp b√†i c·ªßa b·∫°n l∆∞·ª£c b·ªè `<think>`)  \n",
        "- Repo Hugging Face ƒë√£ ch·ª©a **model merged** (folder `hf_repo_merged_model/`)  \n",
        "- Khi n·ªôp: ch·ªâ ƒë∆∞a **t√™n repo** cho h·ªá th·ªëng.\n",
        "\n",
        "Ch√∫c b·∫°n leo leaderboard nhanh üòÑ\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bae23367"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
