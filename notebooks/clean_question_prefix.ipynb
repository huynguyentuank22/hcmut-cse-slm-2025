{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f3ccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Import th√†nh c√¥ng!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Import th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827be805",
   "metadata": {},
   "source": [
    "## 1. ƒê·ªãnh nghƒ©a patterns c·∫ßn lo·∫°i b·ªè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2a3524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Test patterns:\n",
      "================================================================================\n",
      "‚úÖ CLEANED\n",
      "  Before: C√¢u h·ªèi: ƒê√¢y l√† c√¢u h·ªèi test\n",
      "  After:  ƒê√¢y l√† c√¢u h·ªèi test\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: C√¢u h·ªèi : Test v·ªõi space\n",
      "  After:  Test v·ªõi space\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: C√¢u 13: C√¢u h·ªèi s·ªë 13\n",
      "  After:  C√¢u h·ªèi s·ªë 13\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: C√¢u 1 : Test v·ªõi space tr∆∞·ªõc d·∫•u hai ch·∫•m\n",
      "  After:  Test v·ªõi space tr∆∞·ªõc d·∫•u hai ch·∫•m\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: C√¢u 9. C√¢u h·ªèi v·ªõi d·∫•u ch·∫•m\n",
      "  After:  C√¢u h·ªèi v·ªõi d·∫•u ch·∫•m\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: C√¢u 1) C√¢u h·ªèi v·ªõi d·∫•u ngo·∫∑c\n",
      "  After:  C√¢u h·ªèi v·ªõi d·∫•u ngo·∫∑c\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: C√¢u 123) Test\n",
      "  After:  Test\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: Question 6 : Test question\n",
      "  After:  Test question\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: Question 10: Another test\n",
      "  After:  Another test\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: Question: What is this?\n",
      "  After:  What is this?\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: Q 5 : Quick\n",
      "  After:  Quick\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: Q: Quick question\n",
      "  After:  Quick question\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: 1. First question\n",
      "  After:  First question\n",
      "\n",
      "‚úÖ CLEANED\n",
      "  Before: 23) Question with parenthesis\n",
      "  After:  Question with parenthesis\n",
      "\n",
      "‚ÑπÔ∏è  NO CHANGE\n",
      "  Before: ƒê√¢y l√† c√¢u kh√¥ng c√≥ prefix\n",
      "  After:  ƒê√¢y l√† c√¢u kh√¥ng c√≥ prefix\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C√°c patterns c·∫ßn lo·∫°i b·ªè ·ªü ƒë·∫ßu c√¢u h·ªèi\n",
    "PATTERNS_TO_REMOVE = [\n",
    "    r'^C√¢u h·ªèi\\s*:\\s*',           # \"C√¢u h·ªèi: \", \"C√¢u h·ªèi :\"\n",
    "    r'^C√¢u\\s+\\d+\\s*:\\s*',         # \"C√¢u 13: \", \"C√¢u 1 :\", \"C√¢u 123 : \"\n",
    "    r'^C√¢u\\s+\\d+\\.\\s*',           # \"C√¢u 9.\", \"C√¢u 12.\"\n",
    "    r'^C√¢u\\s+\\d+\\)\\s*',           # \"C√¢u 1)\", \"C√¢u 23)\"\n",
    "    r'^Question\\s+\\d+\\s*:\\s*',    # \"Question 6 :\", \"Question 1:\"\n",
    "    r'^Question\\s*:\\s*',          # \"Question: \", \"Question :\"\n",
    "    r'^Q\\s+\\d+\\s*:\\s*',           # \"Q 5 :\", \"Q 1:\"\n",
    "    r'^Q\\s*:\\s*',                 # \"Q: \", \"Q :\"\n",
    "    r'^\\d+\\.\\s*',                 # \"1. \", \"123. \"\n",
    "    r'^\\d+\\)\\s*',                 # \"1) \", \"23) \"\n",
    "]\n",
    "\n",
    "def clean_question_prefix(text):\n",
    "    \"\"\"\n",
    "    Lo·∫°i b·ªè c√°c prefix th·ª´a ·ªü ƒë·∫ßu c√¢u h·ªèi\n",
    "    \"\"\"\n",
    "    original_text = text\n",
    "    \n",
    "    for pattern in PATTERNS_TO_REMOVE:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Strip whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text, text != original_text\n",
    "\n",
    "# Test patterns\n",
    "test_cases = [\n",
    "    \"C√¢u h·ªèi: ƒê√¢y l√† c√¢u h·ªèi test\",\n",
    "    \"C√¢u h·ªèi : Test v·ªõi space\",\n",
    "    \"C√¢u 13: C√¢u h·ªèi s·ªë 13\",\n",
    "    \"C√¢u 1 : Test v·ªõi space tr∆∞·ªõc d·∫•u hai ch·∫•m\",\n",
    "    \"C√¢u 9. C√¢u h·ªèi v·ªõi d·∫•u ch·∫•m\",\n",
    "    \"C√¢u 1) C√¢u h·ªèi v·ªõi d·∫•u ngo·∫∑c\",\n",
    "    \"C√¢u 123) Test\",\n",
    "    \"Question 6 : Test question\",\n",
    "    \"Question 10: Another test\",\n",
    "    \"Question: What is this?\",\n",
    "    \"Q 5 : Quick\",\n",
    "    \"Q: Quick question\",\n",
    "    \"1. First question\",\n",
    "    \"23) Question with parenthesis\",\n",
    "    \"ƒê√¢y l√† c√¢u kh√¥ng c√≥ prefix\"\n",
    "]\n",
    "\n",
    "print(\"üîç Test patterns:\")\n",
    "print(\"=\"*80)\n",
    "for test in test_cases:\n",
    "    cleaned, changed = clean_question_prefix(test)\n",
    "    status = \"‚úÖ CLEANED\" if changed else \"‚ÑπÔ∏è  NO CHANGE\"\n",
    "    print(f\"{status}\")\n",
    "    print(f\"  Before: {test}\")\n",
    "    print(f\"  After:  {cleaned}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a427d",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67633d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Input file: ..\\data\\sft_dataset_vnhsge\\train_sft.jsonl\n",
      "üìÇ Output file: ..\\data\\sft_dataset_vnhsge\\train_sft_cleaned.jsonl\n",
      "\n",
      "‚úÖ Loaded 1573 samples\n"
     ]
    }
   ],
   "source": [
    "# Ch·ªçn file dataset c·∫ßn clean\n",
    "# Uncomment file b·∫°n mu·ªën x·ª≠ l√Ω:\n",
    "\n",
    "# Option 1: VNHSGE dataset\n",
    "input_path = Path('../data/sft_dataset_vnhsge/train_sft.jsonl')\n",
    "output_path = Path('../data/sft_dataset_vnhsge/train_sft_cleaned.jsonl')\n",
    "\n",
    "# Option 2: VMLU dataset\n",
    "# input_path = Path('../data/sft_dataset_vmlu/train_sft_vmlu.jsonl')\n",
    "# output_path = Path('../data/sft_dataset_vmlu/train_sft_vmlu_cleaned.jsonl')\n",
    "\n",
    "# Option 3: Merged final dataset\n",
    "# input_path = Path('../data/train_sft_final.jsonl')\n",
    "# output_path = Path('../data/train_sft_final_cleaned.jsonl')\n",
    "\n",
    "print(f\"üìÇ Input file: {input_path}\")\n",
    "print(f\"üìÇ Output file: {output_path}\")\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412ebe5",
   "metadata": {},
   "source": [
    "## 3. Ph√¢n t√≠ch patterns trong dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b72e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Patterns found in dataset:\n",
      "================================================================================\n",
      "  'C√¢u 10. ': 13 occurrences (0.8%)\n",
      "  'C√¢u 11. ': 12 occurrences (0.8%)\n",
      "  'C√¢u 12. ': 12 occurrences (0.8%)\n",
      "  'C√¢u 13. ': 12 occurrences (0.8%)\n",
      "  'C√¢u 15. ': 12 occurrences (0.8%)\n",
      "  'C√¢u 16. ': 12 occurrences (0.8%)\n",
      "  'C√¢u 3. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 14. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 17. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 19. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 20. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 21. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 24. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 25. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 27. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 29. ': 11 occurrences (0.7%)\n",
      "  'C√¢u 21: ': 11 occurrences (0.7%)\n",
      "  'C√¢u 1. ': 10 occurrences (0.6%)\n",
      "  'C√¢u 2. ': 10 occurrences (0.6%)\n",
      "  'C√¢u 6. ': 10 occurrences (0.6%)\n",
      "  'C√¢u 26. ': 10 occurrences (0.6%)\n",
      "  'C√¢u 28. ': 10 occurrences (0.6%)\n",
      "  'C√¢u 30. ': 10 occurrences (0.6%)\n",
      "  'C√¢u 33. ': 10 occurrences (0.6%)\n",
      "  'C√¢u 39. ': 10 occurrences (0.6%)\n",
      "  'C√¢u 2: ': 10 occurrences (0.6%)\n",
      "  'C√¢u 18: ': 10 occurrences (0.6%)\n",
      "  'C√¢u 20: ': 10 occurrences (0.6%)\n",
      "  'C√¢u 23: ': 10 occurrences (0.6%)\n",
      "  'C√¢u 32: ': 10 occurrences (0.6%)\n",
      "  'C√¢u 7. ': 9 occurrences (0.6%)\n",
      "  'C√¢u 8. ': 9 occurrences (0.6%)\n",
      "  'C√¢u 9. ': 9 occurrences (0.6%)\n",
      "  'C√¢u 18. ': 9 occurrences (0.6%)\n",
      "  'C√¢u 31. ': 9 occurrences (0.6%)\n",
      "  'C√¢u 37. ': 9 occurrences (0.6%)\n",
      "  'C√¢u 1: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 3: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 4: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 6: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 8: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 9: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 10: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 11: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 12: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 13: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 14: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 15: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 16: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 17: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 19: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 22: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 24: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 25: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 26: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 27: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 28: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 29: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 30: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 34: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 38: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 40: ': 9 occurrences (0.6%)\n",
      "  'C√¢u 34. ': 9 occurrences (0.6%)\n",
      "  'C√¢u 4. ': 8 occurrences (0.5%)\n",
      "  'C√¢u 5. ': 8 occurrences (0.5%)\n",
      "  'C√¢u 22. ': 8 occurrences (0.5%)\n",
      "  'C√¢u 23. ': 8 occurrences (0.5%)\n",
      "  'C√¢u 35. ': 8 occurrences (0.5%)\n",
      "  'C√¢u 36. ': 8 occurrences (0.5%)\n",
      "  'C√¢u 38. ': 8 occurrences (0.5%)\n",
      "  'C√¢u 5: ': 8 occurrences (0.5%)\n",
      "  'C√¢u 7: ': 8 occurrences (0.5%)\n",
      "  'C√¢u 31: ': 8 occurrences (0.5%)\n",
      "  'C√¢u 33: ': 8 occurrences (0.5%)\n",
      "  'C√¢u 35: ': 8 occurrences (0.5%)\n",
      "  'C√¢u 36: ': 8 occurrences (0.5%)\n",
      "  'C√¢u 37: ': 8 occurrences (0.5%)\n",
      "  'C√¢u 32. ': 7 occurrences (0.4%)\n",
      "  'C√¢u 40. ': 7 occurrences (0.4%)\n",
      "  'C√¢u 39: ': 6 occurrences (0.4%)\n",
      "  'C√¢u 81. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 82. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 83. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 84. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 85. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 86. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 87. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 88. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 89. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 90. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 91. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 92. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 93. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 94. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 95. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 96. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 97. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 98. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 99. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 100. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 101. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 102. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 103. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 104. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 106. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 107. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 109. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 110. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 113. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 114. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 115. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 117. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 118. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 119. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 120. ': 5 occurrences (0.3%)\n",
      "  'C√¢u 108. ': 4 occurrences (0.3%)\n",
      "  'C√¢u 111. ': 4 occurrences (0.3%)\n",
      "  'C√¢u 105. ': 4 occurrences (0.3%)\n",
      "  'C√¢u 44. ': 4 occurrences (0.3%)\n",
      "  'Question 11: ': 4 occurrences (0.3%)\n",
      "  'Question 12: ': 4 occurrences (0.3%)\n",
      "  'Question 13: ': 4 occurrences (0.3%)\n",
      "  'Question 14: ': 4 occurrences (0.3%)\n",
      "  'Question 15: ': 4 occurrences (0.3%)\n",
      "  'C√¢u 81: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 82: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 83: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 84: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 85: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 86: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 87: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 88: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 89: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 90: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 91: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 92: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 93: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 94: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 95: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 96: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 97: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 98: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 99: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 100: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 101: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 102: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 103: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 104: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 105: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 106: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 107: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 108: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 109: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 110: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 111: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 113: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 114: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 115: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 116: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 117: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 119: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 4 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 5 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 6 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 7 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 8 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 9 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 10 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 11 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 12 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 13 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 14 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 15 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 16 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 17 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 18 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 19 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 20 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 22 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 23 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 24 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 25 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 26 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 28 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 29 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 30 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 31 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 32 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 33 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 34 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 35 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 37 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 38 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 39 : ': 3 occurrences (0.2%)\n",
      "  'C√¢u 41. ': 3 occurrences (0.2%)\n",
      "  'C√¢u 42. ': 3 occurrences (0.2%)\n",
      "  'C√¢u 43. ': 3 occurrences (0.2%)\n",
      "  'C√¢u 45. ': 3 occurrences (0.2%)\n",
      "  'C√¢u 46. ': 3 occurrences (0.2%)\n",
      "  'C√¢u 47. ': 3 occurrences (0.2%)\n",
      "  'C√¢u 48. ': 3 occurrences (0.2%)\n",
      "  'C√¢u 50. ': 3 occurrences (0.2%)\n",
      "  'C√¢u 112. ': 3 occurrences (0.2%)\n",
      "  'C√¢u 116. ': 3 occurrences (0.2%)\n",
      "  'Question 6: ': 3 occurrences (0.2%)\n",
      "  'Question 7: ': 3 occurrences (0.2%)\n",
      "  'Question 8: ': 3 occurrences (0.2%)\n",
      "  'Question 9: ': 3 occurrences (0.2%)\n",
      "  'Question 10: ': 3 occurrences (0.2%)\n",
      "  'Question 17: ': 3 occurrences (0.2%)\n",
      "  'Question 18: ': 3 occurrences (0.2%)\n",
      "  'C√¢u 112: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 118: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 1 : ': 2 occurrences (0.1%)\n",
      "  'C√¢u 2 : ': 2 occurrences (0.1%)\n",
      "  'C√¢u 3 : ': 2 occurrences (0.1%)\n",
      "  'C√¢u 21 : ': 2 occurrences (0.1%)\n",
      "  'C√¢u 27 : ': 2 occurrences (0.1%)\n",
      "  'C√¢u 36 : ': 2 occurrences (0.1%)\n",
      "  'C√¢u 120: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 41: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 42: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 45: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 47: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 49: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 50: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 49. ': 2 occurrences (0.1%)\n",
      "  'Question 16: ': 2 occurrences (0.1%)\n",
      "  'Question 19: ': 2 occurrences (0.1%)\n",
      "  'C√¢u 40 : ': 2 occurrences (0.1%)\n",
      "  'C√¢u 105.  ': 1 occurrences (0.1%)\n",
      "  'C√¢u 43: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 44: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 46: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 48: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 51: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 52: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 53: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 54: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 55: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 56: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 57: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 59: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 60: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 61: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 62: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 63: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 64: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 65: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 66: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 67: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 68: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 69: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 70: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 72: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 73: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 74: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 75: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 76: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 77: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 78: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 79: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 80: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 51. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 52. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 53. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 54. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 55. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 56. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 57. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 58. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 59. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 60. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 61. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 62. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 63. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 64. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 65. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 66. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 67. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 68. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 69. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 70. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 71. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 72. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 74. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 75. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 76. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 77. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 78. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 79. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 80. ': 1 occurrences (0.1%)\n",
      "  'Question 16:  ': 1 occurrences (0.1%)\n",
      "  'Question 6 : ': 1 occurrences (0.1%)\n",
      "  'Question 7 : ': 1 occurrences (0.1%)\n",
      "  'Question 8 : ': 1 occurrences (0.1%)\n",
      "  'Question 9 : ': 1 occurrences (0.1%)\n",
      "  'Question 10 : ': 1 occurrences (0.1%)\n",
      "  'Question 11 : ': 1 occurrences (0.1%)\n",
      "  'Question 12 : ': 1 occurrences (0.1%)\n",
      "  'Question 13 : ': 1 occurrences (0.1%)\n",
      "  'Question 14 : ': 1 occurrences (0.1%)\n",
      "  'Question 15 : ': 1 occurrences (0.1%)\n",
      "  'Question 16 : ': 1 occurrences (0.1%)\n",
      "  'Question 17 : ': 1 occurrences (0.1%)\n",
      "  'Question 18 : ': 1 occurrences (0.1%)\n",
      "  'Question 2: ': 1 occurrences (0.1%)\n",
      "  'Question 3: ': 1 occurrences (0.1%)\n",
      "  'Question 4: ': 1 occurrences (0.1%)\n",
      "  'Question 5: ': 1 occurrences (0.1%)\n",
      "  'C√¢u 1) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 4) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 5) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 6) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 7) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 8) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 9) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 11) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 12) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 13) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 18) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 19) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 20) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 21) ': 1 occurrences (0.1%)\n",
      "  'C√¢u¬† 22. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 23) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 28) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 31) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 33) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 35) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 37) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 38) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 40) ': 1 occurrences (0.1%)\n",
      "  'C√¢u¬† 41) ': 1 occurrences (0.1%)\n",
      "  'C√¢u¬† 42. ': 1 occurrences (0.1%)\n",
      "  'C√¢u 44) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 48) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 49) ': 1 occurrences (0.1%)\n",
      "  'C√¢u 1.\t': 1 occurrences (0.1%)\n",
      "  'C√¢u 2.\t': 1 occurrences (0.1%)\n",
      "  'C√¢u 3.\t': 1 occurrences (0.1%)\n",
      "  'C√¢u 4.\t': 1 occurrences (0.1%)\n",
      "  'C√¢u 5.\t': 1 occurrences (0.1%)\n",
      "  'C√¢u 7.\t': 1 occurrences (0.1%)\n",
      "  'C√¢u 8.\t': 1 occurrences (0.1%)\n",
      "  'C√¢u 9.\t': 1 occurrences (0.1%)\n",
      "  'C√¢u 22.': 1 occurrences (0.1%)\n",
      "  'C√¢u 1. \t': 1 occurrences (0.1%)\n",
      "  'C√¢u 2.  ': 1 occurrences (0.1%)\n",
      "  'C√¢u 4. \t': 1 occurrences (0.1%)\n",
      "  'C√¢u 5. \t': 1 occurrences (0.1%)\n",
      "  'C√¢u 6. \t': 1 occurrences (0.1%)\n",
      "  'C√¢u 8. \t': 1 occurrences (0.1%)\n",
      "\n",
      "üîç Examples with prefix (first 10):\n",
      "  [0] C√¢u 81: C√≥ th·ªÉ s·ª≠ d·ª•ng h√≥a ch·∫•t n√†o sau ƒë√¢y ƒë·ªÉ ph√°t hi·ªán qu√° tr√¨nh h√¥ h·∫•p ·ªü th·ª±c v·∫≠t th·∫£i ra kh√≠ CO2...\n",
      "  [1] C√¢u 82: ƒê·ªông v·∫≠t n√†o sau ƒë√¢y trao ƒë·ªïi kh√≠ v·ªõi m√¥i tr∆∞·ªùng th√¥ng qua h·ªá th·ªëng ·ªëng kh√≠?  ...\n",
      "  [2] C√¢u 83: Axit amin l√† ƒë∆°n ph√¢n c·∫•u t·∫°o n√™n ph√¢n t·ª≠ n√†o sau ƒë√¢y? ...\n",
      "  [3] C√¢u 84: Ph√¢n t·ª≠ n√†o sau ƒë√¢y tr·ª±c ti·∫øp l√†m khu√¥n cho qu√° tr√¨nh d·ªãch m√£?  ...\n",
      "  [4] C√¢u 85: M·ªôt ph√¢n t·ª≠ ADN ·ªü vi khu·∫©n c√≥ 10% s·ªë nucl√™√¥tit lo·∫°i A. Theo l√≠ thuy·∫øt, t·ªâ l·ªá nucl√™√¥tit lo·∫°i ...\n",
      "  [5] C√¢u 86: Theo l√≠ thuy·∫øt, c∆° th·ªÉ c√≥ ki·ªÉu gen aaBB gi·∫£m ph√¢n t·∫°o ra lo·∫°i giao t·ª≠ aB chi·∫øm t·ªâ l·ªá  ...\n",
      "  [6] C√¢u 87: C∆° th·ªÉ c√≥ ki·ªÉu gen n√†o sau ƒë√¢y l√† c∆° th·ªÉ ƒë·ªìng h·ª£p t·ª≠ v·ªÅ t·∫•t c·∫£ c√°c c·∫∑p gen ƒëang x√©t? ...\n",
      "  [7] C√¢u 88: Theo l√≠ thuy·∫øt, ph√©p lai n√†o sau ƒë√¢y cho ƒë·ªùi con c√≥ 1 lo·∫°i ki·ªÉu gen?  ...\n",
      "  [8] C√¢u 89: Theo l√≠ thuy·∫øt, ph√©p lai n√†o sau ƒë√¢y cho ƒë·ªùi con c√≥ ki·ªÉu gen ph√¢n li theo t·ªâ l·ªá 1 : 1 ?  ...\n",
      "  [9] C√¢u 90: Cho bi·∫øt alen D quy ƒë·ªãnh hoa ƒë·ªè tr·ªôi ho√†n to√†n so v·ªõi alen d quy ƒë·ªãnh hoa tr·∫Øng. Theo l√≠ thu...\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch xem c√≥ bao nhi√™u c√¢u h·ªèi c√≥ prefix\n",
    "patterns_found = Counter()\n",
    "samples_with_prefix = []\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    for msg in item['messages']:\n",
    "        if msg['role'] == 'user':\n",
    "            content = msg['content']\n",
    "            # Get first line (question line)\n",
    "            first_line = content.split('\\n')[0]\n",
    "            \n",
    "            # Check each pattern\n",
    "            for pattern in PATTERNS_TO_REMOVE:\n",
    "                match = re.search(pattern, first_line, flags=re.IGNORECASE)\n",
    "                if match:\n",
    "                    patterns_found[match.group(0)] += 1\n",
    "                    if len(samples_with_prefix) < 10:  # Keep first 10 examples\n",
    "                        samples_with_prefix.append((idx, first_line[:100]))\n",
    "                    break\n",
    "\n",
    "print(\"üìä Patterns found in dataset:\")\n",
    "print(\"=\"*80)\n",
    "if patterns_found:\n",
    "    for pattern, count in patterns_found.most_common():\n",
    "        print(f\"  '{pattern}': {count} occurrences ({count/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüîç Examples with prefix (first 10):\")\n",
    "    for idx, sample in samples_with_prefix:\n",
    "        print(f\"  [{idx}] {sample}...\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No patterns found - dataset is already clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade2ed6",
   "metadata": {},
   "source": [
    "## 4. Preview tr∆∞·ªõc v√† sau khi clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc6cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Preview - Tr∆∞·ªõc v√† sau khi clean:\n",
      "================================================================================\n",
      "\n",
      "üìù Sample 0 (ID: MET_Bio_IE_2019_1):\n",
      "  ‚ùå BEFORE: C√¢u 81: C√≥ th·ªÉ s·ª≠ d·ª•ng h√≥a ch·∫•t n√†o sau ƒë√¢y ƒë·ªÉ ph√°t hi·ªán qu√° tr√¨nh h√¥ h·∫•p ·ªü th·ª±c v·∫≠t th·∫£i ra kh√≠ CO2? ...\n",
      "  ‚úÖ AFTER:  C√≥ th·ªÉ s·ª≠ d·ª•ng h√≥a ch·∫•t n√†o sau ƒë√¢y ƒë·ªÉ ph√°t hi·ªán qu√° tr√¨nh h√¥ h·∫•p ·ªü th·ª±c v·∫≠t th·∫£i ra kh√≠ CO2?...\n",
      "\n",
      "üìù Sample 1 (ID: MET_Bio_IE_2019_2):\n",
      "  ‚ùå BEFORE: C√¢u 82: ƒê·ªông v·∫≠t n√†o sau ƒë√¢y trao ƒë·ªïi kh√≠ v·ªõi m√¥i tr∆∞·ªùng th√¥ng qua h·ªá th·ªëng ·ªëng kh√≠?  ...\n",
      "  ‚úÖ AFTER:  ƒê·ªông v·∫≠t n√†o sau ƒë√¢y trao ƒë·ªïi kh√≠ v·ªõi m√¥i tr∆∞·ªùng th√¥ng qua h·ªá th·ªëng ·ªëng kh√≠?...\n",
      "\n",
      "üìù Sample 2 (ID: MET_Bio_IE_2019_3):\n",
      "  ‚ùå BEFORE: C√¢u 83: Axit amin l√† ƒë∆°n ph√¢n c·∫•u t·∫°o n√™n ph√¢n t·ª≠ n√†o sau ƒë√¢y? ...\n",
      "  ‚úÖ AFTER:  Axit amin l√† ƒë∆°n ph√¢n c·∫•u t·∫°o n√™n ph√¢n t·ª≠ n√†o sau ƒë√¢y?...\n",
      "\n",
      "üìù Sample 3 (ID: MET_Bio_IE_2019_4):\n",
      "  ‚ùå BEFORE: C√¢u 84: Ph√¢n t·ª≠ n√†o sau ƒë√¢y tr·ª±c ti·∫øp l√†m khu√¥n cho qu√° tr√¨nh d·ªãch m√£?  ...\n",
      "  ‚úÖ AFTER:  Ph√¢n t·ª≠ n√†o sau ƒë√¢y tr·ª±c ti·∫øp l√†m khu√¥n cho qu√° tr√¨nh d·ªãch m√£?...\n",
      "\n",
      "üìù Sample 4 (ID: MET_Bio_IE_2019_5):\n",
      "  ‚ùå BEFORE: C√¢u 85: M·ªôt ph√¢n t·ª≠ ADN ·ªü vi khu·∫©n c√≥ 10% s·ªë nucl√™√¥tit lo·∫°i A. Theo l√≠ thuy·∫øt, t·ªâ l·ªá nucl√™√¥tit lo·∫°i G c·ªßa  ph√¢n t·ª≠ n√†y l√†  ...\n",
      "  ‚úÖ AFTER:  M·ªôt ph√¢n t·ª≠ ADN ·ªü vi khu·∫©n c√≥ 10% s·ªë nucl√™√¥tit lo·∫°i A. Theo l√≠ thuy·∫øt, t·ªâ l·ªá nucl√™√¥tit lo·∫°i G c·ªßa  ph√¢n t·ª≠ n√†y l√†...\n"
     ]
    }
   ],
   "source": [
    "# Preview 5 samples tr∆∞·ªõc v√† sau khi clean\n",
    "print(\"üîç Preview - Tr∆∞·ªõc v√† sau khi clean:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "preview_count = 0\n",
    "max_previews = 5\n",
    "\n",
    "for idx, item in enumerate(data[:50]):  # Check first 50 samples\n",
    "    if preview_count >= max_previews:\n",
    "        break\n",
    "    \n",
    "    for msg in item['messages']:\n",
    "        if msg['role'] == 'user':\n",
    "            original = msg['content']\n",
    "            lines = original.split('\\n')\n",
    "            first_line = lines[0]\n",
    "            \n",
    "            cleaned_first_line, changed = clean_question_prefix(first_line)\n",
    "            \n",
    "            if changed:\n",
    "                preview_count += 1\n",
    "                print(f\"\\nüìù Sample {idx} (ID: {item.get('id', 'N/A')}):\")\n",
    "                print(f\"  ‚ùå BEFORE: {first_line[:150]}...\")\n",
    "                print(f\"  ‚úÖ AFTER:  {cleaned_first_line[:150]}...\")\n",
    "                \n",
    "                if preview_count >= max_previews:\n",
    "                    break\n",
    "\n",
    "if preview_count == 0:\n",
    "    print(\"\\n‚ÑπÔ∏è  No changes needed - all questions are already clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cbc22",
   "metadata": {},
   "source": [
    "## 5. Clean to√†n b·ªô dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20134b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cleaning dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1573/1573 [00:00<00:00, 66903.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Cleaning completed!\n",
      "\n",
      "üìä Statistics:\n",
      "  Total samples: 1573\n",
      "  Cleaned: 1386 (88.1%)\n",
      "  Unchanged: 187 (11.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean dataset\n",
    "cleaned_data = []\n",
    "stats = {\n",
    "    'total': len(data),\n",
    "    'cleaned': 0,\n",
    "    'unchanged': 0\n",
    "}\n",
    "\n",
    "print(\"üîÑ Cleaning dataset...\")\n",
    "for item in tqdm(data):\n",
    "    cleaned_item = item.copy()\n",
    "    item_changed = False\n",
    "    \n",
    "    for msg in cleaned_item['messages']:\n",
    "        if msg['role'] == 'user':\n",
    "            original_content = msg['content']\n",
    "            lines = original_content.split('\\n')\n",
    "            \n",
    "            # Clean first line (question)\n",
    "            if lines:\n",
    "                cleaned_first_line, changed = clean_question_prefix(lines[0])\n",
    "                if changed:\n",
    "                    lines[0] = cleaned_first_line\n",
    "                    msg['content'] = '\\n'.join(lines)\n",
    "                    item_changed = True\n",
    "    \n",
    "    cleaned_data.append(cleaned_item)\n",
    "    \n",
    "    if item_changed:\n",
    "        stats['cleaned'] += 1\n",
    "    else:\n",
    "        stats['unchanged'] += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Cleaning completed!\")\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"  Total samples: {stats['total']}\")\n",
    "print(f\"  Cleaned: {stats['cleaned']} ({stats['cleaned']/stats['total']*100:.1f}%)\")\n",
    "print(f\"  Unchanged: {stats['unchanged']} ({stats['unchanged']/stats['total']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ed029",
   "metadata": {},
   "source": [
    "## 6. Save cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf1127ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved cleaned dataset to: ..\\data\\sft_dataset_vnhsge\\train_sft_cleaned.jsonl\n",
      "üì¶ File size: 1.07 MB\n",
      "üìä Total samples: 1573\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for item in cleaned_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Saved cleaned dataset to: {output_path}\")\n",
    "print(f\"üì¶ File size: {output_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(f\"üìä Total samples: {len(cleaned_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b00ec",
   "metadata": {},
   "source": [
    "## 7. Verify cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3b6b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Verify successful!\n",
      "üìä Loaded 1573 samples from cleaned file\n",
      "üìä Matches original count: True\n",
      "\n",
      "‚úÖ All patterns removed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Verify cleaned file\n",
    "verify_data = []\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        verify_data.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"‚úÖ Verify successful!\")\n",
    "print(f\"üìä Loaded {len(verify_data)} samples from cleaned file\")\n",
    "print(f\"üìä Matches original count: {len(verify_data) == len(data)}\")\n",
    "\n",
    "# Check if any patterns still exist\n",
    "remaining_patterns = Counter()\n",
    "for item in verify_data:\n",
    "    for msg in item['messages']:\n",
    "        if msg['role'] == 'user':\n",
    "            first_line = msg['content'].split('\\n')[0]\n",
    "            for pattern in PATTERNS_TO_REMOVE:\n",
    "                match = re.search(pattern, first_line, flags=re.IGNORECASE)\n",
    "                if match:\n",
    "                    remaining_patterns[match.group(0)] += 1\n",
    "                    break\n",
    "\n",
    "if remaining_patterns:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Some patterns still exist:\")\n",
    "    for pattern, count in remaining_patterns.most_common():\n",
    "        print(f\"  '{pattern}': {count} occurrences\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All patterns removed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d6f3b",
   "metadata": {},
   "source": [
    "## 8. Compare samples side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aebe5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Side-by-side comparison (samples that changed):\n",
      "================================================================================\n",
      "\n",
      "‚ÑπÔ∏è  No changes detected\n"
     ]
    }
   ],
   "source": [
    "# Show side-by-side comparison for samples that changed\n",
    "print(\"üîç Side-by-side comparison (samples that changed):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "shown = 0\n",
    "max_show = 10\n",
    "\n",
    "for orig, cleaned in zip(data, verify_data):\n",
    "    if shown >= max_show:\n",
    "        break\n",
    "    \n",
    "    orig_user = None\n",
    "    cleaned_user = None\n",
    "    \n",
    "    for msg in orig['messages']:\n",
    "        if msg['role'] == 'user':\n",
    "            orig_user = msg['content']\n",
    "            break\n",
    "    \n",
    "    for msg in cleaned['messages']:\n",
    "        if msg['role'] == 'user':\n",
    "            cleaned_user = msg['content']\n",
    "            break\n",
    "    \n",
    "    if orig_user != cleaned_user:\n",
    "        shown += 1\n",
    "        print(f\"\\nüìù Sample {shown} (ID: {orig.get('id', 'N/A')}):\")\n",
    "        print(f\"\\n  ‚ùå ORIGINAL:\")\n",
    "        print(f\"     {orig_user[:200]}...\")\n",
    "        print(f\"\\n  ‚úÖ CLEANED:\")\n",
    "        print(f\"     {cleaned_user[:200]}...\")\n",
    "        print(f\"\\n  {'-'*76}\")\n",
    "\n",
    "if shown == 0:\n",
    "    print(\"\\n‚ÑπÔ∏è  No changes detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf5345",
   "metadata": {},
   "source": [
    "## 9. Batch clean multiple files (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a54a9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Uncomment code above ƒë·ªÉ clean nhi·ªÅu files c√πng l√∫c\n"
     ]
    }
   ],
   "source": [
    "# Uncomment ƒë·ªÉ clean nhi·ªÅu files c√πng l√∫c\n",
    "\"\"\"\n",
    "files_to_clean = [\n",
    "    ('../data/sft_dataset_vnhsge/train_sft.jsonl', '../data/sft_dataset_vnhsge/train_sft_cleaned.jsonl'),\n",
    "    ('../data/sft_dataset_vmlu/train_sft_vmlu.jsonl', '../data/sft_dataset_vmlu/train_sft_vmlu_cleaned.jsonl'),\n",
    "    ('../data/train_sft_final.jsonl', '../data/train_sft_final_cleaned.jsonl'),\n",
    "]\n",
    "\n",
    "for input_file, output_file in files_to_clean:\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Skipping {input_path} - file not found\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüîÑ Processing: {input_path}\")\n",
    "    \n",
    "    # Load\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    \n",
    "    # Clean\n",
    "    cleaned_data = []\n",
    "    cleaned_count = 0\n",
    "    \n",
    "    for item in tqdm(data, desc=\"Cleaning\"):\n",
    "        cleaned_item = item.copy()\n",
    "        item_changed = False\n",
    "        \n",
    "        for msg in cleaned_item['messages']:\n",
    "            if msg['role'] == 'user':\n",
    "                original_content = msg['content']\n",
    "                lines = original_content.split('\\n')\n",
    "                \n",
    "                if lines:\n",
    "                    cleaned_first_line, changed = clean_question_prefix(lines[0])\n",
    "                    if changed:\n",
    "                        lines[0] = cleaned_first_line\n",
    "                        msg['content'] = '\\n'.join(lines)\n",
    "                        item_changed = True\n",
    "        \n",
    "        cleaned_data.append(cleaned_item)\n",
    "        if item_changed:\n",
    "            cleaned_count += 1\n",
    "    \n",
    "    # Save\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in cleaned_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"  ‚úÖ Saved to: {output_path}\")\n",
    "    print(f\"  üìä Cleaned: {cleaned_count}/{len(data)} samples ({cleaned_count/len(data)*100:.1f}%)\")\n",
    "\"\"\"\n",
    "print(\"üí° Uncomment code above ƒë·ªÉ clean nhi·ªÅu files c√πng l√∫c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589e146",
   "metadata": {},
   "source": [
    "## üìã T·ªïng k·∫øt\n",
    "\n",
    "### K·∫øt qu·∫£:\n",
    "- ‚úÖ ƒê√£ lo·∫°i b·ªè c√°c prefix th·ª´a ·ªü ƒë·∫ßu c√¢u h·ªèi\n",
    "- ‚úÖ ƒê√£ l∆∞u dataset ƒë√£ clean\n",
    "- ‚úÖ ƒê√£ verify k·∫øt qu·∫£\n",
    "\n",
    "### Patterns ƒë√£ x·ª≠ l√Ω:\n",
    "- `C√¢u h·ªèi: `\n",
    "- `C√¢u 13: ` (C√¢u + s·ªë + d·∫•u hai ch·∫•m)\n",
    "- `Question: `\n",
    "- `Q: `\n",
    "- `1. ` (s·ªë + d·∫•u ch·∫•m)\n",
    "\n",
    "### File output:\n",
    "- Dataset ƒë√£ clean ƒë∆∞·ª£c l∆∞u v·ªõi suffix `_cleaned.jsonl`\n",
    "- Gi·ªØ nguy√™n format v√† structure\n",
    "- Ch·ªâ thay ƒë·ªïi ph·∫ßn user content (c√¢u h·ªèi)\n",
    "\n",
    "### B∆∞·ªõc ti·∫øp theo:\n",
    "1. ‚úÖ S·ª≠ d·ª•ng file `_cleaned.jsonl` cho training\n",
    "2. üîÑ C√≥ th·ªÉ merge cleaned datasets\n",
    "3. üéØ Apply th√™m augmentation ho·∫∑c reasoning\n",
    "\n",
    "### L∆∞u √Ω:\n",
    "- File g·ªëc ƒë∆∞·ª£c gi·ªØ nguy√™n\n",
    "- C√≥ th·ªÉ d·ªÖ d√†ng quay l·∫°i n·∫øu c·∫ßn\n",
    "- Patterns c√≥ th·ªÉ t√πy ch·ªânh trong bi·∫øn `PATTERNS_TO_REMOVE`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
