{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de28be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce46558",
   "metadata": {},
   "source": [
    "## 1. Ph√¢n t√≠ch Strategy theo t·ª´ng m√¥n h·ªçc\n",
    "\n",
    "M·ªói m√¥n h·ªçc c√≥ c√°ch suy lu·∫≠n kh√°c nhau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1311e663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìö Mathematics: To√°n h·ªçc - Gi·∫£i t·ª´ng b∆∞·ªõc, c√¥ng th·ª©c, t√≠nh to√°n\n",
      "============================================================\n",
      "  1. X√°c ƒë·ªãnh d·∫°ng b√†i to√°n v√† c√¥ng th·ª©c c·∫ßn d√πng\n",
      "  2. Li·ªát k√™ c√°c d·ªØ ki·ªán ƒë√£ cho\n",
      "  3. Th·ª±c hi·ªán c√°c b∆∞·ªõc t√≠nh to√°n\n",
      "  4. Ki·ªÉm tra v√† k·∫øt lu·∫≠n\n",
      "\n",
      "============================================================\n",
      "üìö Physics: V·∫≠t l√Ω - Ph√¢n t√≠ch hi·ªán t∆∞·ª£ng, √°p d·ª•ng ƒë·ªãnh lu·∫≠t\n",
      "============================================================\n",
      "  1. X√°c ƒë·ªãnh hi·ªán t∆∞·ª£ng v·∫≠t l√Ω v√† ƒë·ªãnh lu·∫≠t li√™n quan\n",
      "  2. Ph√¢n t√≠ch c√°c ƒë·∫°i l∆∞·ª£ng v√† m·ªëi quan h·ªá\n",
      "  3. √Åp d·ª•ng c√¥ng th·ª©c ho·∫∑c ƒë·ªãnh lu·∫≠t\n",
      "  4. So s√°nh v·ªõi c√°c ƒë√°p √°n v√† k·∫øt lu·∫≠n\n",
      "\n",
      "============================================================\n",
      "üìö Chemistry: H√≥a h·ªçc - Ph∆∞∆°ng tr√¨nh, c√¢n b·∫±ng, t√≠nh to√°n mol\n",
      "============================================================\n",
      "  1. X√°c ƒë·ªãnh ch·∫•t tham gia v√† ph·∫£n ·ª©ng h√≥a h·ªçc\n",
      "  2. Vi·∫øt v√† c√¢n b·∫±ng ph∆∞∆°ng tr√¨nh (n·∫øu c√≥)\n",
      "  3. T√≠nh to√°n theo mol, kh·ªëi l∆∞·ª£ng ho·∫∑c th·ªÉ t√≠ch\n",
      "  4. Ph√¢n t√≠ch t√≠nh ch·∫•t v√† k·∫øt lu·∫≠n\n",
      "\n",
      "============================================================\n",
      "üìö Biology: Sinh h·ªçc - Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm, ch·ª©c nƒÉng sinh h·ªçc\n",
      "============================================================\n",
      "  1. X√°c ƒë·ªãnh kh√°i ni·ªám ho·∫∑c qu√° tr√¨nh sinh h·ªçc\n",
      "  2. Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm v√† ch·ª©c nƒÉng\n",
      "  3. So s√°nh c√°c ƒë√°p √°n v·ªõi ki·∫øn th·ª©c sinh h·ªçc\n",
      "  4. Lo·∫°i tr·ª´ c√°c ƒë√°p √°n sai v√† k·∫øt lu·∫≠n\n",
      "\n",
      "============================================================\n",
      "üìö Geography: ƒê·ªãa l√Ω - Ph√¢n t√≠ch v·ªã tr√≠, ƒë·∫∑c ƒëi·ªÉm ƒë·ªãa l√Ω\n",
      "============================================================\n",
      "  1. X√°c ƒë·ªãnh v√πng ƒë·ªãa l√Ω ho·∫∑c hi·ªán t∆∞·ª£ng\n",
      "  2. Ph√¢n t√≠ch c√°c y·∫øu t·ªë ƒë·ªãa l√Ω (kh√≠ h·∫≠u, ƒë·ªãa h√¨nh, t√†i nguy√™n)\n",
      "  3. So s√°nh v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm trong ƒë√°p √°n\n",
      "  4. K·∫øt lu·∫≠n d·ª±a tr√™n ki·∫øn th·ª©c ƒë·ªãa l√Ω\n",
      "\n",
      "============================================================\n",
      "üìö History: L·ªãch s·ª≠ - Ph√¢n t√≠ch s·ª± ki·ªán, nguy√™n nh√¢n, h·∫≠u qu·∫£\n",
      "============================================================\n",
      "  1. X√°c ƒë·ªãnh s·ª± ki·ªán l·ªãch s·ª≠ v√† th·ªùi gian\n",
      "  2. Ph√¢n t√≠ch nguy√™n nh√¢n v√† b·ªëi c·∫£nh\n",
      "  3. Xem x√©t h·∫≠u qu·∫£ v√† √Ω nghƒ©a\n",
      "  4. So s√°nh v·ªõi c√°c ƒë√°p √°n v√† k·∫øt lu·∫≠n\n",
      "\n",
      "============================================================\n",
      "üìö Literature: Ng·ªØ vƒÉn - Ph√¢n t√≠ch vƒÉn h·ªçc, tu t·ª´, √Ω nghƒ©a\n",
      "============================================================\n",
      "  1. X√°c ƒë·ªãnh t√°c ph·∫©m, t√°c gi·∫£ ho·∫∑c ƒëo·∫°n vƒÉn\n",
      "  2. Ph√¢n t√≠ch n·ªôi dung, h√¨nh th·ª©c v√† ngh·ªá thu·∫≠t\n",
      "  3. Xem x√©t bi·ªán ph√°p tu t·ª´ v√† gi√° tr·ªã vƒÉn h·ªçc\n",
      "  4. K·∫øt lu·∫≠n v·ªÅ √Ω nghƒ©a v√† th√¥ng ƒëi·ªáp\n",
      "\n",
      "============================================================\n",
      "üìö CivicEducation: Gi√°o d·ª•c c√¥ng d√¢n - Ph√¢n t√≠ch lu·∫≠t ph√°p, ƒë·∫°o ƒë·ª©c\n",
      "============================================================\n",
      "  1. X√°c ƒë·ªãnh kh√°i ni·ªám ho·∫∑c quy ƒë·ªãnh ph√°p lu·∫≠t\n",
      "  2. Ph√¢n t√≠ch quy·ªÅn, nghƒ©a v·ª• v√† tr√°ch nhi·ªám\n",
      "  3. So s√°nh v·ªõi c√°c gi√° tr·ªã ƒë·∫°o ƒë·ª©c v√† x√£ h·ªôi\n",
      "  4. K·∫øt lu·∫≠n d·ª±a tr√™n chu·∫©n m·ª±c ph√°p l√Ω v√† ƒë·∫°o ƒë·ª©c\n",
      "\n",
      "============================================================\n",
      "üìö English: Ti·∫øng Anh - Grammar, vocabulary, context\n",
      "============================================================\n",
      "  1. Identify the grammar rule or vocabulary context\n",
      "  2. Analyze the sentence structure and meaning\n",
      "  3. Evaluate each option against the context\n",
      "  4. Select the most appropriate answer\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a reasoning strategy cho t·ª´ng m√¥n\n",
    "\n",
    "REASONING_STRATEGIES = {\n",
    "    \"Mathematics\": {\n",
    "        \"description\": \"To√°n h·ªçc - Gi·∫£i t·ª´ng b∆∞·ªõc, c√¥ng th·ª©c, t√≠nh to√°n\",\n",
    "        \"steps\": [\n",
    "            \"X√°c ƒë·ªãnh d·∫°ng b√†i to√°n v√† c√¥ng th·ª©c c·∫ßn d√πng\",\n",
    "            \"Li·ªát k√™ c√°c d·ªØ ki·ªán ƒë√£ cho\",\n",
    "            \"Th·ª±c hi·ªán c√°c b∆∞·ªõc t√≠nh to√°n\",\n",
    "            \"Ki·ªÉm tra v√† k·∫øt lu·∫≠n\"\n",
    "        ],\n",
    "        \"keywords\": [\"t√≠nh\", \"ph∆∞∆°ng tr√¨nh\", \"c√¥ng th·ª©c\", \"bi·ªÉu th·ª©c\", \"gi·∫£i\", \"t√¨m\", \"ƒë·∫°o h√†m\", \"t√≠ch ph√¢n\"]\n",
    "    },\n",
    "    \n",
    "    \"Physics\": {\n",
    "        \"description\": \"V·∫≠t l√Ω - Ph√¢n t√≠ch hi·ªán t∆∞·ª£ng, √°p d·ª•ng ƒë·ªãnh lu·∫≠t\",\n",
    "        \"steps\": [\n",
    "            \"X√°c ƒë·ªãnh hi·ªán t∆∞·ª£ng v·∫≠t l√Ω v√† ƒë·ªãnh lu·∫≠t li√™n quan\",\n",
    "            \"Ph√¢n t√≠ch c√°c ƒë·∫°i l∆∞·ª£ng v√† m·ªëi quan h·ªá\",\n",
    "            \"√Åp d·ª•ng c√¥ng th·ª©c ho·∫∑c ƒë·ªãnh lu·∫≠t\",\n",
    "            \"So s√°nh v·ªõi c√°c ƒë√°p √°n v√† k·∫øt lu·∫≠n\"\n",
    "        ],\n",
    "        \"keywords\": [\"ƒë·ªãnh lu·∫≠t\", \"l·ª±c\", \"nƒÉng l∆∞·ª£ng\", \"chuy·ªÉn ƒë·ªông\", \"ƒëi·ªán\", \"t·ª´\", \"quang\", \"nhi·ªát\"]\n",
    "    },\n",
    "    \n",
    "    \"Chemistry\": {\n",
    "        \"description\": \"H√≥a h·ªçc - Ph∆∞∆°ng tr√¨nh, c√¢n b·∫±ng, t√≠nh to√°n mol\",\n",
    "        \"steps\": [\n",
    "            \"X√°c ƒë·ªãnh ch·∫•t tham gia v√† ph·∫£n ·ª©ng h√≥a h·ªçc\",\n",
    "            \"Vi·∫øt v√† c√¢n b·∫±ng ph∆∞∆°ng tr√¨nh (n·∫øu c√≥)\",\n",
    "            \"T√≠nh to√°n theo mol, kh·ªëi l∆∞·ª£ng ho·∫∑c th·ªÉ t√≠ch\",\n",
    "            \"Ph√¢n t√≠ch t√≠nh ch·∫•t v√† k·∫øt lu·∫≠n\"\n",
    "        ],\n",
    "        \"keywords\": [\"ph·∫£n ·ª©ng\", \"ch·∫•t\", \"mol\", \"kh·ªëi l∆∞·ª£ng\", \"dung d·ªãch\", \"axit\", \"baz∆°\", \"oxi h√≥a\", \"kh·ª≠\"]\n",
    "    },\n",
    "    \n",
    "    \"Biology\": {\n",
    "        \"description\": \"Sinh h·ªçc - Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm, ch·ª©c nƒÉng sinh h·ªçc\",\n",
    "        \"steps\": [\n",
    "            \"X√°c ƒë·ªãnh kh√°i ni·ªám ho·∫∑c qu√° tr√¨nh sinh h·ªçc\",\n",
    "            \"Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm v√† ch·ª©c nƒÉng\",\n",
    "            \"So s√°nh c√°c ƒë√°p √°n v·ªõi ki·∫øn th·ª©c sinh h·ªçc\",\n",
    "            \"Lo·∫°i tr·ª´ c√°c ƒë√°p √°n sai v√† k·∫øt lu·∫≠n\"\n",
    "        ],\n",
    "        \"keywords\": [\"t·∫ø b√†o\", \"gen\", \"di truy·ªÅn\", \"sinh s·∫£n\", \"h√¥ h·∫•p\", \"quang h·ª£p\", \"ti·∫øn h√≥a\", \"sinh th√°i\"]\n",
    "    },\n",
    "    \n",
    "    \"Geography\": {\n",
    "        \"description\": \"ƒê·ªãa l√Ω - Ph√¢n t√≠ch v·ªã tr√≠, ƒë·∫∑c ƒëi·ªÉm ƒë·ªãa l√Ω\",\n",
    "        \"steps\": [\n",
    "            \"X√°c ƒë·ªãnh v√πng ƒë·ªãa l√Ω ho·∫∑c hi·ªán t∆∞·ª£ng\",\n",
    "            \"Ph√¢n t√≠ch c√°c y·∫øu t·ªë ƒë·ªãa l√Ω (kh√≠ h·∫≠u, ƒë·ªãa h√¨nh, t√†i nguy√™n)\",\n",
    "            \"So s√°nh v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm trong ƒë√°p √°n\",\n",
    "            \"K·∫øt lu·∫≠n d·ª±a tr√™n ki·∫øn th·ª©c ƒë·ªãa l√Ω\"\n",
    "        ],\n",
    "        \"keywords\": [\"v√πng\", \"kh√≠ h·∫≠u\", \"ƒë·ªãa h√¨nh\", \"t√†i nguy√™n\", \"d√¢n c∆∞\", \"kinh t·∫ø\", \"m√¥i tr∆∞·ªùng\"]\n",
    "    },\n",
    "    \n",
    "    \"History\": {\n",
    "        \"description\": \"L·ªãch s·ª≠ - Ph√¢n t√≠ch s·ª± ki·ªán, nguy√™n nh√¢n, h·∫≠u qu·∫£\",\n",
    "        \"steps\": [\n",
    "            \"X√°c ƒë·ªãnh s·ª± ki·ªán l·ªãch s·ª≠ v√† th·ªùi gian\",\n",
    "            \"Ph√¢n t√≠ch nguy√™n nh√¢n v√† b·ªëi c·∫£nh\",\n",
    "            \"Xem x√©t h·∫≠u qu·∫£ v√† √Ω nghƒ©a\",\n",
    "            \"So s√°nh v·ªõi c√°c ƒë√°p √°n v√† k·∫øt lu·∫≠n\"\n",
    "        ],\n",
    "        \"keywords\": [\"s·ª± ki·ªán\", \"th·ªùi k·ª≥\", \"chi·∫øn tranh\", \"c√°ch m·∫°ng\", \"phong tr√†o\", \"tri·ªÅu ƒë·∫°i\", \"nƒÉm\"]\n",
    "    },\n",
    "    \n",
    "    \"Literature\": {\n",
    "        \"description\": \"Ng·ªØ vƒÉn - Ph√¢n t√≠ch vƒÉn h·ªçc, tu t·ª´, √Ω nghƒ©a\",\n",
    "        \"steps\": [\n",
    "            \"X√°c ƒë·ªãnh t√°c ph·∫©m, t√°c gi·∫£ ho·∫∑c ƒëo·∫°n vƒÉn\",\n",
    "            \"Ph√¢n t√≠ch n·ªôi dung, h√¨nh th·ª©c v√† ngh·ªá thu·∫≠t\",\n",
    "            \"Xem x√©t bi·ªán ph√°p tu t·ª´ v√† gi√° tr·ªã vƒÉn h·ªçc\",\n",
    "            \"K·∫øt lu·∫≠n v·ªÅ √Ω nghƒ©a v√† th√¥ng ƒëi·ªáp\"\n",
    "        ],\n",
    "        \"keywords\": [\"t√°c ph·∫©m\", \"t√°c gi·∫£\", \"th∆°\", \"vƒÉn xu√¥i\", \"bi·ªán ph√°p\", \"h√¨nh t∆∞·ª£ng\", \"ch·ªß ƒë·ªÅ\"]\n",
    "    },\n",
    "    \n",
    "    \"CivicEducation\": {\n",
    "        \"description\": \"Gi√°o d·ª•c c√¥ng d√¢n - Ph√¢n t√≠ch lu·∫≠t ph√°p, ƒë·∫°o ƒë·ª©c\",\n",
    "        \"steps\": [\n",
    "            \"X√°c ƒë·ªãnh kh√°i ni·ªám ho·∫∑c quy ƒë·ªãnh ph√°p lu·∫≠t\",\n",
    "            \"Ph√¢n t√≠ch quy·ªÅn, nghƒ©a v·ª• v√† tr√°ch nhi·ªám\",\n",
    "            \"So s√°nh v·ªõi c√°c gi√° tr·ªã ƒë·∫°o ƒë·ª©c v√† x√£ h·ªôi\",\n",
    "            \"K·∫øt lu·∫≠n d·ª±a tr√™n chu·∫©n m·ª±c ph√°p l√Ω v√† ƒë·∫°o ƒë·ª©c\"\n",
    "        ],\n",
    "        \"keywords\": [\"quy·ªÅn\", \"nghƒ©a v·ª•\", \"ph√°p lu·∫≠t\", \"hi·∫øn ph√°p\", \"ƒë·∫°o ƒë·ª©c\", \"c√¥ng d√¢n\", \"x√£ h·ªôi\"]\n",
    "    },\n",
    "    \n",
    "    \"English\": {\n",
    "        \"description\": \"Ti·∫øng Anh - Grammar, vocabulary, context\",\n",
    "        \"steps\": [\n",
    "            \"Identify the grammar rule or vocabulary context\",\n",
    "            \"Analyze the sentence structure and meaning\",\n",
    "            \"Evaluate each option against the context\",\n",
    "            \"Select the most appropriate answer\"\n",
    "        ],\n",
    "        \"keywords\": [\"grammar\", \"tense\", \"vocabulary\", \"preposition\", \"phrase\", \"clause\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Hi·ªÉn th·ªã strategies\n",
    "for subject, strategy in REASONING_STRATEGIES.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìö {subject}: {strategy['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for i, step in enumerate(strategy['steps'], 1):\n",
    "        print(f\"  {i}. {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce90f6",
   "metadata": {},
   "source": [
    "## 2. Load v√† ph√¢n t√≠ch d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4f1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1573 samples\n",
      "\n",
      "Subject distribution:\n",
      "English             :  250 samples\n",
      "CivicEducation      :  200 samples\n",
      "History             :  200 samples\n",
      "Chemistry           :  196 samples\n",
      "Biology             :  190 samples\n",
      "Geography           :  190 samples\n",
      "Mathematics         :  180 samples\n",
      "Physics             :  167 samples\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "input_file = Path(\"../data/sft_dataset_vnhsge/train_sft.jsonl\")\n",
    "\n",
    "data = []\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} samples\")\n",
    "\n",
    "# Ph√¢n t√≠ch ph√¢n ph·ªëi m√¥n h·ªçc\n",
    "from collections import Counter\n",
    "\n",
    "subjects = [item['subject'] for item in data]\n",
    "subject_counts = Counter(subjects)\n",
    "\n",
    "print(\"\\nSubject distribution:\")\n",
    "for subject, count in sorted(subject_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{subject:20s}: {count:4d} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8041a",
   "metadata": {},
   "source": [
    "## 3. C√°c template reasoning theo m√¥n h·ªçc\n",
    "\n",
    "T·∫°o templates c·ª• th·ªÉ cho t·ª´ng m√¥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034691f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing reasoning generation:\n",
      "\n",
      "Subject: Biology\n",
      "Question: C√¢u 81: C√≥ th·ªÉ s·ª≠ d·ª•ng h√≥a ch·∫•t n√†o sau ƒë√¢y ƒë·ªÉ ph√°t hi·ªán qu√° tr√¨nh h√¥ h·∫•p ·ªü th·ª±c v·∫≠t th·∫£i ra kh√≠ CO2...\n",
      "\n",
      "Generated reasoning preview (first 300 chars):\n",
      "<think>\n",
      "X√°c ƒë·ªãnh kh√°i ni·ªám/qu√° tr√¨nh sinh h·ªçc\n",
      "Ph√¢n t√≠ch c·∫•u tr√∫c v√† ch·ª©c nƒÉng\n",
      "So s√°nh c√°c ƒë√°p √°n v·ªõi ki·∫øn th·ª©c sinh h·ªçc\n",
      "ƒê√°p √°n B ch√≠nh x√°c\n",
      "</think>\n"
     ]
    }
   ],
   "source": [
    "def generate_reasoning_template(subject: str, question: str, options: Dict[str, str], correct_answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate reasoning template based on subject\n",
    "    \n",
    "    Args:\n",
    "        subject: M√¥n h·ªçc\n",
    "        question: C√¢u h·ªèi\n",
    "        options: Dict c·ªßa c√°c options {\"A\": \"...\", \"B\": \"...\", ...}\n",
    "        correct_answer: ƒê√°p √°n ƒë√∫ng (\"A\", \"B\", \"C\", \"D\")\n",
    "    \n",
    "    Returns:\n",
    "        Reasoning text v·ªõi <think> tags\n",
    "    \"\"\"\n",
    "    \n",
    "    if subject == \"Mathematics\":\n",
    "        reasoning = f\"\"\"<think>\n",
    "X√°c ƒë·ªãnh d·∫°ng b√†i v√† c√¥ng th·ª©c c·∫ßn d√πng\n",
    "Ph√¢n t√≠ch d·ªØ ki·ªán v√† m·ªëi quan h·ªá\n",
    "√Åp d·ª•ng c√¥ng th·ª©c v√† t√≠nh to√°n\n",
    "ƒê√°p √°n {correct_answer} ph√π h·ª£p v·ªõi k·∫øt qu·∫£\n",
    "</think>\"\"\"\n",
    "    \n",
    "    elif subject == \"Physics\":\n",
    "        reasoning = f\"\"\"<think>\n",
    "X√°c ƒë·ªãnh hi·ªán t∆∞·ª£ng v·∫≠t l√Ω v√† ƒë·ªãnh lu·∫≠t li√™n quan\n",
    "Ph√¢n t√≠ch c√°c ƒë·∫°i l∆∞·ª£ng v√† m·ªëi quan h·ªá\n",
    "√Åp d·ª•ng ƒë·ªãnh lu·∫≠t v√† t√≠nh to√°n\n",
    "ƒê√°p √°n {correct_answer} ƒë√∫ng theo ƒë·ªãnh lu·∫≠t v·∫≠t l√Ω\n",
    "</think>\"\"\"\n",
    "    \n",
    "    elif subject == \"Chemistry\":\n",
    "        reasoning = f\"\"\"<think>\n",
    "X√°c ƒë·ªãnh ch·∫•t tham gia v√† lo·∫°i ph·∫£n ·ª©ng\n",
    "Ph√¢n t√≠ch t√≠nh ch·∫•t h√≥a h·ªçc\n",
    "Vi·∫øt v√† c√¢n b·∫±ng ph∆∞∆°ng tr√¨nh (n·∫øu c·∫ßn)\n",
    "ƒê√°p √°n {correct_answer} ph√π h·ª£p v·ªõi t√≠nh ch·∫•t/ph·∫£n ·ª©ng\n",
    "</think>\"\"\"\n",
    "    \n",
    "    elif subject == \"Biology\":\n",
    "        reasoning = f\"\"\"<think>\n",
    "X√°c ƒë·ªãnh kh√°i ni·ªám/qu√° tr√¨nh sinh h·ªçc\n",
    "Ph√¢n t√≠ch c·∫•u tr√∫c v√† ch·ª©c nƒÉng\n",
    "So s√°nh c√°c ƒë√°p √°n v·ªõi ki·∫øn th·ª©c sinh h·ªçc\n",
    "ƒê√°p √°n {correct_answer} ch√≠nh x√°c\n",
    "</think>\"\"\"\n",
    "    \n",
    "    elif subject == \"Geography\":\n",
    "        reasoning = f\"\"\"<think>\n",
    "X√°c ƒë·ªãnh v√πng ƒë·ªãa l√Ω ho·∫∑c hi·ªán t∆∞·ª£ng\n",
    "Ph√¢n t√≠ch y·∫øu t·ªë ƒë·ªãa l√Ω (kh√≠ h·∫≠u, ƒë·ªãa h√¨nh, t√†i nguy√™n)\n",
    "So s√°nh v·ªõi ƒë·∫∑c ƒëi·ªÉm c√°c ƒë√°p √°n\n",
    "ƒê√°p √°n {correct_answer} ƒë√∫ng\n",
    "</think>\"\"\"\n",
    "    \n",
    "    elif subject == \"History\":\n",
    "        reasoning = f\"\"\"<think>\n",
    "X√°c ƒë·ªãnh s·ª± ki·ªán, th·ªùi k·ª≥ v√† b·ªëi c·∫£nh l·ªãch s·ª≠\n",
    "Ph√¢n t√≠ch nguy√™n nh√¢n v√† di·ªÖn bi·∫øn\n",
    "Xem x√©t h·∫≠u qu·∫£ v√† √Ω nghƒ©a\n",
    "ƒê√°p √°n {correct_answer} ph√π h·ª£p v·ªõi s·ª± ki·ªán l·ªãch s·ª≠\n",
    "</think>\"\"\"\n",
    "    \n",
    "    elif subject == \"Literature\":\n",
    "        reasoning = f\"\"\"<think>\n",
    "X√°c ƒë·ªãnh t√°c ph·∫©m, t√°c gi·∫£ v√† th·ªÉ lo·∫°i\n",
    "Ph√¢n t√≠ch n·ªôi dung, ngh·ªá thu·∫≠t v√† bi·ªán ph√°p tu t·ª´\n",
    "ƒê√°nh gi√° gi√° tr·ªã vƒÉn h·ªçc v√† th√¥ng ƒëi·ªáp\n",
    "ƒê√°p √°n {correct_answer} ph√π h·ª£p\n",
    "</think>\"\"\"\n",
    "    \n",
    "    elif subject == \"CivicEducation\":\n",
    "        reasoning = f\"\"\"<think>\n",
    "X√°c ƒë·ªãnh kh√°i ni·ªám ho·∫∑c quy ƒë·ªãnh ph√°p lu·∫≠t\n",
    "Ph√¢n t√≠ch quy·ªÅn, nghƒ©a v·ª• v√† tr√°ch nhi·ªám\n",
    "So s√°nh v·ªõi chu·∫©n m·ª±c ph√°p l√Ω v√† ƒë·∫°o ƒë·ª©c\n",
    "ƒê√°p √°n {correct_answer} ƒë√∫ng theo quy ƒë·ªãnh\n",
    "</think>\"\"\"\n",
    "    \n",
    "    elif subject == \"English\":\n",
    "        reasoning = f\"\"\"<think>\n",
    "Identify question type (grammar/vocabulary)\n",
    "Analyze sentence structure and context\n",
    "Evaluate each option against the rule\n",
    "Answer {correct_answer} is correct\n",
    "</think>\"\"\"\n",
    "    \n",
    "    else:\n",
    "        # Generic template for unknown subjects\n",
    "        reasoning = f\"\"\"<think>\n",
    "Ph√¢n t√≠ch c√¢u h·ªèi v√† y√™u c·∫ßu\n",
    "Xem x√©t v√† so s√°nh c√°c ƒë√°p √°n\n",
    "Lo·∫°i tr·ª´ ƒë√°p √°n kh√¥ng ph√π h·ª£p\n",
    "ƒê√°p √°n {correct_answer} l√† ƒë√°p √°n ƒë√∫ng\n",
    "</think>\"\"\"\n",
    "    \n",
    "    return reasoning.strip()\n",
    "\n",
    "\n",
    "# Test v·ªõi m·ªôt v√†i samples\n",
    "print(\"\\nTesting reasoning generation:\\n\")\n",
    "test_sample = data[0]\n",
    "print(f\"Subject: {test_sample['subject']}\")\n",
    "print(f\"Question: {test_sample['messages'][1]['content'][:100]}...\")\n",
    "print(f\"\\nGenerated reasoning preview (first 300 chars):\")\n",
    "\n",
    "# Parse question to get options\n",
    "def parse_question(content):\n",
    "    lines = content.strip().split('\\n')\n",
    "    question_parts = []\n",
    "    options = {}\n",
    "    expected_labels = ['A', 'B', 'C', 'D']\n",
    "    current_label_idx = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Match c·∫£ c√≥ v√† kh√¥ng c√≥ d·∫•u ch·∫•m: A. text ho·∫∑c A text\n",
    "        match = re.match(r'^([A-D])[\\.\\:\\s]*\\s*(.*)', line)\n",
    "        if match:\n",
    "            label = match.group(1)\n",
    "            text = match.group(2).strip()\n",
    "            \n",
    "            # Skip n·∫øu text r·ªóng\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            # Auto-fix: Lu√¥n d√πng label theo th·ª© t·ª± expected\n",
    "            if current_label_idx < len(expected_labels):\n",
    "                actual_label = expected_labels[current_label_idx]\n",
    "                \n",
    "                if label in options or label != actual_label:\n",
    "                    # print(f\"‚ö†Ô∏è Warning: Found '{label}', using '{actual_label}' instead\")\n",
    "                    label = actual_label\n",
    "                \n",
    "                options[label] = text\n",
    "                current_label_idx += 1\n",
    "        else:\n",
    "            # N·∫øu kh√¥ng ph·∫£i option, coi l√† ph·∫ßn question\n",
    "            question_parts.append(line)\n",
    "    \n",
    "    return ' '.join(question_parts), options\n",
    "\n",
    "q, opts = parse_question(test_sample['messages'][1]['content'])\n",
    "correct = json.loads(test_sample['messages'][2]['content'])['answer']\n",
    "reasoning = generate_reasoning_template(test_sample['subject'], q, opts, correct)\n",
    "print(reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1317e",
   "metadata": {},
   "source": [
    "## 4. Augment dataset v·ªõi reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c71335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reasoning_to_dataset(input_file: Path, output_file: Path, sample_ratio: float = 1.0):\n",
    "    \"\"\"\n",
    "    Th√™m reasoning v√†o dataset\n",
    "    \n",
    "    Args:\n",
    "        input_file: File JSONL ƒë·∫ßu v√†o\n",
    "        output_file: File JSONL ƒë·∫ßu ra\n",
    "        sample_ratio: T·ª∑ l·ªá samples c·∫ßn th√™m reasoning (0.0-1.0)\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Loaded {len(data)} samples\")\n",
    "    \n",
    "    # Process data\n",
    "    augmented_data = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for item in tqdm(data, desc=\"Adding reasoning\"):\n",
    "        try:\n",
    "            # Parse question\n",
    "            user_content = item['messages'][1]['content']\n",
    "            question, options = parse_question(user_content)\n",
    "            \n",
    "            if len(options) != 4:\n",
    "                # Gi·ªØ nguy√™n n·∫øu kh√¥ng parse ƒë∆∞·ª£c\n",
    "                augmented_data.append(item)\n",
    "                print(item)\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Get correct answer\n",
    "            correct_answer = json.loads(item['messages'][2]['content'])['answer']\n",
    "            \n",
    "            # Decide whether to add reasoning\n",
    "            if random.random() > sample_ratio:\n",
    "                # Gi·ªØ nguy√™n\n",
    "                augmented_data.append(item)\n",
    "                continue\n",
    "            \n",
    "            # Generate reasoning\n",
    "            reasoning = generate_reasoning_template(\n",
    "                subject=item['subject'],\n",
    "                question=question,\n",
    "                options=options,\n",
    "                correct_answer=correct_answer\n",
    "            )\n",
    "            \n",
    "            # ‚úÖ NEW FORMAT: reasoning n·∫±m ngo√†i JSON\n",
    "            # Format: <think>...</think>\\n{\"answer\":\"A\"}\n",
    "            answer_json = json.dumps({\"answer\": correct_answer}, ensure_ascii=False)\n",
    "            new_response_content = f\"{reasoning}\\n{answer_json}\"\n",
    "            \n",
    "            # Create new item\n",
    "            new_item = {\n",
    "                \"messages\": [\n",
    "                    item['messages'][0],  # System message\n",
    "                    item['messages'][1],  # User message\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": new_response_content\n",
    "                    }\n",
    "                ],\n",
    "                \"id\": item['id'],\n",
    "                \"subject\": item['subject']\n",
    "            }\n",
    "            \n",
    "            augmented_data.append(new_item)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {item.get('id', 'unknown')}: {e}\")\n",
    "            augmented_data.append(item)  # Gi·ªØ nguy√™n n·∫øu l·ªói\n",
    "            print(item)\n",
    "            skipped += 1\n",
    "    \n",
    "    print(f\"\\nProcessed {len(augmented_data)} samples\")\n",
    "    print(f\"Skipped: {skipped}\")\n",
    "    print(f\"Added reasoning to: {len(augmented_data) - skipped} samples\")\n",
    "    \n",
    "    # Save\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in augmented_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"\\nSaved to {output_file}\")\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1dfdea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1573 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding reasoning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1573/1573 [00:00<00:00, 74379.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1573 samples\n",
      "Skipped: 0\n",
      "Added reasoning to: 1573 samples\n",
      "\n",
      "Saved to ..\\data\\sft_dataset_vnhsge\\train_sft_with_reasoning.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Th·ª±c hi·ªán augmentation\n",
    "output_file = Path(\"../data/sft_dataset_vnhsge/train_sft_with_reasoning.jsonl\")\n",
    "\n",
    "augmented_data = add_reasoning_to_dataset(\n",
    "    input_file=input_file,\n",
    "    output_file=output_file,\n",
    "    sample_ratio=1.0  # 1.0 = th√™m reasoning cho 100% samples, 0.5 = 50%, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893587b",
   "metadata": {},
   "source": [
    "## 5. Ki·ªÉm tra k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d4d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Sample data with reasoning:\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üìö Subject: Mathematics\n",
      "ID: MET_Math_IE_2019_1\n",
      "================================================================================\n",
      "\n",
      "üìù Question:\n",
      "C√¢u 1) Th·ªÉ t√≠ch c·ªßa kh·ªëi l·∫≠p ph∆∞∆°ng c·∫°nh 2a b·∫±ng:\n",
      "A. 8a^3. \n",
      "B. 2a^3. \n",
      "C. a^3.¬†¬† \n",
      "D. 6a^3....\n",
      "\n",
      "ü§î Assistant Response:\n",
      "<think>\n",
      "X√°c ƒë·ªãnh d·∫°ng b√†i v√† c√¥ng th·ª©c c·∫ßn d√πng\n",
      "Ph√¢n t√≠ch d·ªØ ki·ªán v√† m·ªëi quan h·ªá\n",
      "√Åp d·ª•ng c√¥ng th·ª©c v√† t√≠nh to√°n\n",
      "ƒê√°p √°n A ph√π h·ª£p v·ªõi k·∫øt qu·∫£\n",
      "</think>\n",
      "{\"answer\": \"A\"}...\n",
      "\n",
      "================================================================================\n",
      "üìö Subject: Physics\n",
      "ID: MET_Phy_IE_2019_1\n",
      "================================================================================\n",
      "\n",
      "üìù Question:\n",
      "C√¢u 1. \tM·ªôt v·∫≠t dao ƒë·ªông ƒëi·ªÅu ho√† theo ph∆∞∆°ng tr√¨nh: \n",
      "x = Acos(\\omega*t+\\varphi) v·ªõi A > 0 v√† omega > 0. Pha c·ªßa dao ƒë·ªông ·ªü th·ªùi ƒëi·ªÉm t l√† \n",
      "A. \\omega\n",
      "B. cos(\\omega*t+ \\varphi)\n",
      "C. \\omega*t+ \\varphi\n",
      "D. ...\n",
      "\n",
      "ü§î Assistant Response:\n",
      "<think>\n",
      "X√°c ƒë·ªãnh hi·ªán t∆∞·ª£ng v·∫≠t l√Ω v√† ƒë·ªãnh lu·∫≠t li√™n quan\n",
      "Ph√¢n t√≠ch c√°c ƒë·∫°i l∆∞·ª£ng v√† m·ªëi quan h·ªá\n",
      "√Åp d·ª•ng ƒë·ªãnh lu·∫≠t v√† t√≠nh to√°n\n",
      "ƒê√°p √°n C ƒë√∫ng theo ƒë·ªãnh lu·∫≠t v·∫≠t l√Ω\n",
      "</think>\n",
      "{\"answer\": \"C\"}...\n",
      "\n",
      "================================================================================\n",
      "üìö Subject: Chemistry\n",
      "ID: MET_Chem_IE_2019_1\n",
      "================================================================================\n",
      "\n",
      "üìù Question:\n",
      "C√¢u 41: ·ªû ƒëi·ªÅu ki·ªán th∆∞·ªùng, kim lo·∫°i n√†o sau ƒë√¢y ·ªü tr·∫°ng th√°i l·ªèng?\n",
      "A. ${Zn}$.\n",
      "B. ${Hg}$.\n",
      "C. ${Ag}$.\n",
      "D. ${Cu}$....\n",
      "\n",
      "ü§î Assistant Response:\n",
      "<think>\n",
      "X√°c ƒë·ªãnh ch·∫•t tham gia v√† lo·∫°i ph·∫£n ·ª©ng\n",
      "Ph√¢n t√≠ch t√≠nh ch·∫•t h√≥a h·ªçc\n",
      "Vi·∫øt v√† c√¢n b·∫±ng ph∆∞∆°ng tr√¨nh (n·∫øu c·∫ßn)\n",
      "ƒê√°p √°n B ph√π h·ª£p v·ªõi t√≠nh ch·∫•t/ph·∫£n ·ª©ng\n",
      "</think>\n",
      "{\"answer\": \"B\"}...\n",
      "\n",
      "================================================================================\n",
      "üìö Subject: Biology\n",
      "ID: MET_Bio_IE_2019_1\n",
      "================================================================================\n",
      "\n",
      "üìù Question:\n",
      "C√¢u 81: C√≥ th·ªÉ s·ª≠ d·ª•ng h√≥a ch·∫•t n√†o sau ƒë√¢y ƒë·ªÉ ph√°t hi·ªán qu√° tr√¨nh h√¥ h·∫•p ·ªü th·ª±c v·∫≠t th·∫£i ra kh√≠ CO2? \n",
      "A. Dung d·ªãch NaCl. \n",
      "B. Dung d·ªãch Ca(OH)2. \n",
      "C. Dung d·ªãch KCl. \n",
      "D. Dung d·ªãch H2SO4....\n",
      "\n",
      "ü§î Assistant Response:\n",
      "<think>\n",
      "X√°c ƒë·ªãnh kh√°i ni·ªám/qu√° tr√¨nh sinh h·ªçc\n",
      "Ph√¢n t√≠ch c·∫•u tr√∫c v√† ch·ª©c nƒÉng\n",
      "So s√°nh c√°c ƒë√°p √°n v·ªõi ki·∫øn th·ª©c sinh h·ªçc\n",
      "ƒê√°p √°n B ch√≠nh x√°c\n",
      "</think>\n",
      "{\"answer\": \"B\"}...\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra m·ªôt v√†i samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample data with reasoning:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group by subject\n",
    "from collections import defaultdict\n",
    "samples_by_subject = defaultdict(list)\n",
    "for item in augmented_data:\n",
    "    samples_by_subject[item['subject']].append(item)\n",
    "\n",
    "# Show one sample per subject\n",
    "for subject in [\"Mathematics\", \"Physics\", \"Chemistry\", \"Biology\"]:\n",
    "    if subject in samples_by_subject:\n",
    "        sample = samples_by_subject[subject][0]\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìö Subject: {subject}\")\n",
    "        print(f\"ID: {sample['id']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nüìù Question:\\n{sample['messages'][1]['content'][:200]}...\")\n",
    "        print(f\"\\nü§î Assistant Response:\")\n",
    "        response_content = sample['messages'][2]['content']\n",
    "        \n",
    "        # Check if response has reasoning (contains <think>)\n",
    "        if '<think>' in response_content:\n",
    "            print(response_content[:500] + \"...\")\n",
    "        else:\n",
    "            # Old format or no reasoning\n",
    "            try:\n",
    "                response = json.loads(response_content)\n",
    "                print(f\"Answer: {response['answer']}\")\n",
    "            except:\n",
    "                print(response_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc128bae",
   "metadata": {},
   "source": [
    "## 6. C·∫≠p nh·∫≠t system prompt\n",
    "\n",
    "Khi training v·ªõi reasoning, c·∫ßn c·∫≠p nh·∫≠t system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d10d06dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New system prompt:\n",
      "B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi tr·∫Øc nghi·ªám th√¥ng minh. \n",
      "H√£y suy lu·∫≠n t·ª´ng b∆∞·ªõc trong th·∫ª <think>, sau ƒë√≥ tr·∫£ JSON v·ªõi ƒë√°p √°n.\n",
      "Format: \n",
      "<think>B∆∞·ªõc 1: ...\n",
      "B∆∞·ªõc 2: ...\n",
      "K·∫øt lu·∫≠n: ...</think>\n",
      "{\"answer\":\"A\"}\n",
      "\n",
      "================================================================================\n",
      "Updated system prompt and saved to ..\\data\\sft_dataset_vnhsge\\train_sft_with_reasoning.jsonl\n"
     ]
    }
   ],
   "source": [
    "# System prompt m·ªõi\n",
    "NEW_SYSTEM_PROMPT = \"\"\"B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi tr·∫Øc nghi·ªám th√¥ng minh. \n",
    "H√£y suy lu·∫≠n t·ª´ng b∆∞·ªõc trong th·∫ª <think>, sau ƒë√≥ tr·∫£ JSON v·ªõi ƒë√°p √°n.\n",
    "Format: \n",
    "<think>B∆∞·ªõc 1: ...\n",
    "B∆∞·ªõc 2: ...\n",
    "K·∫øt lu·∫≠n: ...</think>\n",
    "{\"answer\":\"A\"}\"\"\"\n",
    "\n",
    "print(\"New system prompt:\")\n",
    "print(NEW_SYSTEM_PROMPT)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Update system message in all samples\n",
    "def update_system_prompt(data, new_prompt):\n",
    "    for item in data:\n",
    "        item['messages'][0]['content'] = new_prompt\n",
    "    return data\n",
    "\n",
    "# Apply\n",
    "augmented_data = update_system_prompt(augmented_data, NEW_SYSTEM_PROMPT)\n",
    "\n",
    "# Save again\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in augmented_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Updated system prompt and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c724be",
   "metadata": {},
   "source": [
    "## 7. Statistics v√† ph√¢n t√≠ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c06140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with reasoning: 1573\n",
      "\n",
      "Reasoning length statistics:\n",
      "  Mean: 159 characters\n",
      "  Median: 157 characters\n",
      "  Min: 147 characters\n",
      "  Max: 172 characters\n",
      "\n",
      "Total length (question + reasoning) statistics:\n",
      "  Mean: 516 characters\n",
      "  Max: 3272 characters\n",
      "\n",
      "Estimated average tokens: ~129 tokens\n"
     ]
    }
   ],
   "source": [
    "# T√≠nh to√°n statistics\n",
    "import numpy as np\n",
    "\n",
    "reasoning_lengths = []\n",
    "total_lengths = []\n",
    "\n",
    "for item in augmented_data:\n",
    "    response_content = item['messages'][2]['content']\n",
    "    \n",
    "    # Check if response has reasoning (new format)\n",
    "    if '<think>' in response_content:\n",
    "        # Extract reasoning part (from <think> to </think>)\n",
    "        import re\n",
    "        match = re.search(r'<think>.*?</think>', response_content, re.DOTALL)\n",
    "        if match:\n",
    "            reasoning_text = match.group(0)\n",
    "            reasoning_lengths.append(len(reasoning_text))\n",
    "            total_lengths.append(len(item['messages'][1]['content']) + len(reasoning_text))\n",
    "\n",
    "print(f\"Samples with reasoning: {len(reasoning_lengths)}\")\n",
    "print(f\"\\nReasoning length statistics:\")\n",
    "if reasoning_lengths:\n",
    "    print(f\"  Mean: {np.mean(reasoning_lengths):.0f} characters\")\n",
    "    print(f\"  Median: {np.median(reasoning_lengths):.0f} characters\")\n",
    "    print(f\"  Min: {np.min(reasoning_lengths):.0f} characters\")\n",
    "    print(f\"  Max: {np.max(reasoning_lengths):.0f} characters\")\n",
    "\n",
    "    print(f\"\\nTotal length (question + reasoning) statistics:\")\n",
    "    print(f\"  Mean: {np.mean(total_lengths):.0f} characters\")\n",
    "    print(f\"  Max: {np.max(total_lengths):.0f} characters\")\n",
    "\n",
    "    # ∆Ø·ªõc t√≠nh tokens (roughly 1 token ~ 4 characters for Vietnamese)\n",
    "    avg_tokens = np.mean(total_lengths) / 4\n",
    "    print(f\"\\nEstimated average tokens: ~{avg_tokens:.0f} tokens\")\n",
    "else:\n",
    "    print(\"No samples with reasoning found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39623b64",
   "metadata": {},
   "source": [
    "## 8. T·∫°o mixed dataset (c√≥ v√† kh√¥ng c√≥ reasoning)\n",
    "\n",
    "C√≥ th·ªÉ t·∫°o dataset k·∫øt h·ª£p ƒë·ªÉ model h·ªçc c·∫£ hai c√°ch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4a69fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mixed dataset (50% with reasoning, 50% without)...\n",
      "Loaded 1573 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding reasoning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1573/1573 [00:00<00:00, 56284.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1573 samples\n",
      "Skipped: 0\n",
      "Added reasoning to: 1573 samples\n",
      "\n",
      "Saved to ..\\data\\sft_dataset_vnhsge\\train_sft_mixed_reasoning.jsonl\n",
      "\n",
      "Mixed dataset statistics:\n",
      "  Total samples: 1573\n",
      "  With reasoning: 786 (50.0%)\n",
      "  Without reasoning: 787 (50.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o mixed dataset: 50% c√≥ reasoning, 50% kh√¥ng c√≥\n",
    "mixed_output = Path(\"../data/sft_dataset_vnhsge/train_sft_mixed_reasoning.jsonl\")\n",
    "\n",
    "print(\"Creating mixed dataset (50% with reasoning, 50% without)...\")\n",
    "augmented_mixed = add_reasoning_to_dataset(\n",
    "    input_file=input_file,\n",
    "    output_file=mixed_output,\n",
    "    sample_ratio=0.5  # 50% c√≥ reasoning\n",
    ")\n",
    "\n",
    "# Count samples with reasoning (new format)\n",
    "with_reasoning = sum(1 for item in augmented_mixed \n",
    "                     if '<think>' in item['messages'][2]['content'])\n",
    "print(f\"\\nMixed dataset statistics:\")\n",
    "print(f\"  Total samples: {len(augmented_mixed)}\")\n",
    "print(f\"  With reasoning: {with_reasoning} ({with_reasoning/len(augmented_mixed)*100:.1f}%)\")\n",
    "print(f\"  Without reasoning: {len(augmented_mixed)-with_reasoning} ({(len(augmented_mixed)-with_reasoning)/len(augmented_mixed)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7041b",
   "metadata": {},
   "source": [
    "## T·ªïng k·∫øt\n",
    "\n",
    "**Files ƒë√£ t·∫°o:**\n",
    "1. `train_sft_with_reasoning.jsonl` - 100% samples c√≥ reasoning\n",
    "2. `train_sft_mixed_reasoning.jsonl` - 50% c√≥ reasoning, 50% kh√¥ng\n",
    "\n",
    "**L·ª£i √≠ch:**\n",
    "- Model h·ªçc c√°ch suy lu·∫≠n c√≥ c·∫•u tr√∫c\n",
    "- TƒÉng t√≠nh gi·∫£i th√≠ch ƒë∆∞·ª£c\n",
    "- C√≥ th·ªÉ c·∫£i thi·ªán accuracy nh·ªù reasoning steps\n",
    "\n",
    "**L∆∞u √Ω khi training:**\n",
    "- C·∫ßn tƒÉng `max_seq_length` v√¨ c√≥ th√™m reasoning\n",
    "- C√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh `batch_size` do sequence d√†i h∆°n\n",
    "- N√™n d√πng `completion_only_loss=True` ƒë·ªÉ ch·ªâ t√≠nh loss tr√™n reasoning + answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
